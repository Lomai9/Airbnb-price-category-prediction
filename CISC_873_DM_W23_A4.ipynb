{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9c7d40c561824e9ca10a7ac96bf5d712": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_04040bf4af6748169f661887bf780094",
              "IPY_MODEL_7f05579a3c664b52900084ecec591c39",
              "IPY_MODEL_fa08d086a33f4957b7e50b189ea2c41f"
            ],
            "layout": "IPY_MODEL_324f6f92b60a4259a5763a670cf52280"
          }
        },
        "04040bf4af6748169f661887bf780094": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f1c617ce9bc34098be63688060bfd740",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_00b00d030ed94195a52d9177364ed7bb",
            "value": "100%"
          }
        },
        "7f05579a3c664b52900084ecec591c39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3382a412020e4a89beaecded09170935",
            "max": 7627,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8c0a683b1a084cb7b3d593e0aef6377a",
            "value": 7627
          }
        },
        "fa08d086a33f4957b7e50b189ea2c41f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e84099137d34a6d863e82cf68c5b1ad",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_dbb591f7b63f4b2395a91a5598bcb0f4",
            "value": " 7627/7627 [37:01&lt;00:00,  3.70it/s]"
          }
        },
        "324f6f92b60a4259a5763a670cf52280": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1c617ce9bc34098be63688060bfd740": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "00b00d030ed94195a52d9177364ed7bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3382a412020e4a89beaecded09170935": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c0a683b1a084cb7b3d593e0aef6377a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0e84099137d34a6d863e82cf68c5b1ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dbb591f7b63f4b2395a91a5598bcb0f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "375415aa9072474aad0c166993c87605": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_06f2392f856d44a48def359c260ea50d",
              "IPY_MODEL_c62ae040005e432b826344db98130b1b",
              "IPY_MODEL_406ce9e619f640e68e6249e41207bce3"
            ],
            "layout": "IPY_MODEL_361d310eb58a4faf9303ad7660b93e9f"
          }
        },
        "06f2392f856d44a48def359c260ea50d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d105eda59a946b392df43b288fb6fbe",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_6c7f443cd60e4c4e947e71d65876917f",
            "value": "100%"
          }
        },
        "c62ae040005e432b826344db98130b1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c85eb13c4a824a3a83886356280147cc",
            "max": 7360,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ed2886d0ce6b4bafa7a6f16a43551c95",
            "value": 7360
          }
        },
        "406ce9e619f640e68e6249e41207bce3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c4cbe1e9f494e0a998556c131d7b46b",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_fc6e8c9f75e44cc5a3bed80f94a05815",
            "value": " 7360/7360 [34:33&lt;00:00,  3.52it/s]"
          }
        },
        "361d310eb58a4faf9303ad7660b93e9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d105eda59a946b392df43b288fb6fbe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c7f443cd60e4c4e947e71d65876917f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c85eb13c4a824a3a83886356280147cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed2886d0ce6b4bafa7a6f16a43551c95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1c4cbe1e9f494e0a998556c131d7b46b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc6e8c9f75e44cc5a3bed80f94a05815": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ‚úîÔ∏è Problem Formulation:\n",
        "**The problem: ‚ùé**\n",
        "\n",
        "One of the biggest problems when people prepare to post a new listing on airbnb is, how much should one ask for? So in this assignmen,tbuild neural network model model to predict the listing price based on the listing characteristics.\n",
        "\n",
        "**What is the input? ‚è©**\n",
        "\n",
        "dataset contains listings of different areas in Montreal during 2019.\n",
        "summary for this real estate and its image.\n",
        "\n",
        "**What is the output? ‚è™**\n",
        "\n",
        "Predict the type of this real estate and its price.\n",
        "\n",
        "**What data mining function is required? ü§î**\n",
        "\n",
        "-Multi Classification methods.\n",
        "-Lemmatizer function.\n",
        "-load images function.\n",
        "-the data mining function required to text preprocessing is tokenization and vectorization each text.\n",
        "\n",
        "**What could be the challenges? ‚õè**\n",
        "\n",
        "\n",
        "*   train data set include 301 null values in summary column.\n",
        "\n",
        "*   The summary column includes values which are in different languages, So we need to tranlate all values into english.\n",
        "\n",
        "*  clean text and using lemmatization method.\n",
        "*  preprocessing image columns.\n",
        "*  build different models and train it, that is take a lot of time.\n",
        "*  multi-prediction problem.\n",
        "*  dealing with transfer learning.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**What is the impact? üòÄ**\n",
        "\n",
        "-Predict the price and the type of place by summary and image of place.\n",
        "\n",
        "-This prediction can be useful in the case of buying and selling because it sets appropriate prices\n",
        "\n",
        "**What is an ideal solution?**‚úä\n",
        "\n",
        "The last trail when using Transfer learning Using (VGG model).  \n",
        "this method has faster training speed, fewer training samples per time\n",
        "This Trail Get Score on Kaggle (0.67527) (no. epoch =5)\n",
        "The score without Lammatization (0.67934) (no. epoch =10)\n",
        "\n",
        "On the other hand, its main disadvantage is that it is very slow to train if trained from scratch. Even on a decent GPU."
      ],
      "metadata": {
        "id": "fuc4TBY7w3-Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experimental protocol üíª\n",
        "-Import some modules to dealing with data set\n",
        "\n",
        "-load dataset from CSV file\n",
        "\n",
        "-Data Exploration \n",
        "\n",
        "-Preparations (Check the data if it's Clean or not and clean it if it's not chean)\n",
        "- Text cleaning\n",
        "*   translator.\n",
        "*   lemmatization method.\n",
        "-Data Preprocessing \n",
        "*   Image data: resize\n",
        "*   Text data: tokenization and converting to integer IDs\n",
        "\n",
        "-building models (seven trail) \n",
        "\n",
        "\n",
        "1.   Using Conv2d layer (Multi-modality learning with Multi-objective learning )\n",
        "2.   Using LSTM and Droupout layer (Multi-modality learning with Multi-objective learning )\n",
        "3.   Using BiDirectional layer and GRU layer (Multi-modality learning with Multi-objective learning )\n",
        "4.   Using LSTM (Multi-modality learning and Single task)\n",
        "5.   Using LSTM layer (Multi-objective learning (multi-task)(predicting both price and type)\n",
        "6.   Using Conv2d layer. (Multi-objective learning (multi-task))\n",
        "7.   Transfer learning (VGG model) (Multi-modality learning with Multi-objective learning)\n",
        "-Model Training\n",
        "-Predition\n",
        "-Create Submit file and check the score of each model on kaggle."
      ],
      "metadata": {
        "id": "vP8YmIznxfnj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpY5h8jZtGM1"
      },
      "source": [
        "# Import some modules to dealing with data set ‚ñ∂"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**loading all relevant modules and setting some options:**"
      ],
      "metadata": {
        "id": "L9Vs3FY_K6_i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9rQmhjiCXDz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import numpy as np # linear algebra\n",
        "import matplotlib.pyplot as plt #visualizations \n",
        "import seaborn as sns\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "#For dealing with text related tasks, we will be using nltk. The terrific scikit-learn library will be used to handle tasks related to machine learning.\n",
        "import nltk \n",
        "\n",
        "#nlp\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18j8IidrtbUY"
      },
      "source": [
        "#load dataset from CSV file ‚è¨"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#to load the dataset from drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3sammPO2uMSw",
        "outputId": "1db79b4f-c9d2-449b-91dd-1d479ad06940"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BVqtACTZtcC-"
      },
      "outputs": [],
      "source": [
        "#read dataset into files \n",
        "train= pd.read_csv('/content/drive/MyDrive/train_xy.csv') #train dataset\n",
        "test= pd.read_csv('/content/drive/MyDrive/test_x.csv') #test dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15YLD6vTuM5d"
      },
      "source": [
        "#Data Exploration üîç\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmY49ETAuPYg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f4676f2-9514-4514-f802-d2f3746311e3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             summary            image  \\\n",
              "0  Spacious, sunny and cozy modern apartment in t...  img_train/0.jpg   \n",
              "1  Located in one of the most vibrant and accessi...  img_train/1.jpg   \n",
              "2  Logement coquet et douillet √† 10 minutes du ce...  img_train/2.jpg   \n",
              "3  Beautiful and spacious (1076 sc ft, / 100 mc) ...  img_train/3.jpg   \n",
              "4  Tr√®s grand appartement ''rustique'' et tr√®s ag...  img_train/4.jpg   \n",
              "\n",
              "        type  price  \n",
              "0  Apartment      1  \n",
              "1  Apartment      0  \n",
              "2  Apartment      1  \n",
              "3  Apartment      1  \n",
              "4  Apartment      0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2bbfde7b-cb32-4abf-837f-e1285c7c2f16\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>summary</th>\n",
              "      <th>image</th>\n",
              "      <th>type</th>\n",
              "      <th>price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Spacious, sunny and cozy modern apartment in t...</td>\n",
              "      <td>img_train/0.jpg</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Located in one of the most vibrant and accessi...</td>\n",
              "      <td>img_train/1.jpg</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Logement coquet et douillet √† 10 minutes du ce...</td>\n",
              "      <td>img_train/2.jpg</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Beautiful and spacious (1076 sc ft, / 100 mc) ...</td>\n",
              "      <td>img_train/3.jpg</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Tr√®s grand appartement ''rustique'' et tr√®s ag...</td>\n",
              "      <td>img_train/4.jpg</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2bbfde7b-cb32-4abf-837f-e1285c7c2f16')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2bbfde7b-cb32-4abf-837f-e1285c7c2f16 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2bbfde7b-cb32-4abf-837f-e1285c7c2f16');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "#display the train set\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7o8iAu9quSHd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2615eca-7971-4ce1-cc30-ec8c5a4b4da7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   id                                            summary           image\n",
              "0   0  Charming warm house is ready to host you here ...  img_test/0.jpg\n",
              "1   1  La chambre est spacieuse et lumineuse, dans un...  img_test/1.jpg\n",
              "2   2  Grande chambre confortable situ√©e au sous-sol ...  img_test/2.jpg"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-58f02220-b649-4eb6-9636-fc86d24f5743\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>summary</th>\n",
              "      <th>image</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Charming warm house is ready to host you here ...</td>\n",
              "      <td>img_test/0.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>La chambre est spacieuse et lumineuse, dans un...</td>\n",
              "      <td>img_test/1.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Grande chambre confortable situ√©e au sous-sol ...</td>\n",
              "      <td>img_test/2.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-58f02220-b649-4eb6-9636-fc86d24f5743')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-58f02220-b649-4eb6-9636-fc86d24f5743 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-58f02220-b649-4eb6-9636-fc86d24f5743');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "#display the first three row of the test set\n",
        "test.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EzhlGK-Uuji3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecf37906-037e-4d34-e428-25555765970c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7627, 4)\n",
            "(7360, 3)\n"
          ]
        }
      ],
      "source": [
        "print(train.shape)\n",
        "print(test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Preparations"
      ],
      "metadata": {
        "id": "SnSE7-8NcyAY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jc_AX1NQAGVT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "907755aa-f498-4316-909d-8cb99c9b21af"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "summary    301\n",
              "image        0\n",
              "type         0\n",
              "price        0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "#check the null values (train set)\n",
        "train.isnull().sum()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#check the null values (test set)\n",
        "test.isnull().sum()\n"
      ],
      "metadata": {
        "id": "r2xBD-GibqcE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da42064a-dd02-4bb7-d496-ca881c4f35e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id         0\n",
              "summary    0\n",
              "image      0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 301 null values in summary column in train data set. \n",
        "we should dealing with it."
      ],
      "metadata": {
        "id": "Ye1ZaFwf5YJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Drop null values which in summary column in train data\n",
        "train.dropna()"
      ],
      "metadata": {
        "id": "0V0rs4BObt80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "80fccd11-b08a-4fc0-e6b6-4dc1af14bebf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                summary               image  \\\n",
              "0     Spacious, sunny and cozy modern apartment in t...     img_train/0.jpg   \n",
              "1     Located in one of the most vibrant and accessi...     img_train/1.jpg   \n",
              "2     Logement coquet et douillet √† 10 minutes du ce...     img_train/2.jpg   \n",
              "3     Beautiful and spacious (1076 sc ft, / 100 mc) ...     img_train/3.jpg   \n",
              "4     Tr√®s grand appartement ''rustique'' et tr√®s ag...     img_train/4.jpg   \n",
              "...                                                 ...                 ...   \n",
              "7622  Un grand logement 4 et 1/2, tout inclut, bien ...  img_train/7626.jpg   \n",
              "7623  Magnificent condo directly on the river. You w...  img_train/7627.jpg   \n",
              "7624  This apartment is perfect for anyone visiting ...  img_train/7628.jpg   \n",
              "7625  It is a cozy ,clean ,and comfortable apartment...  img_train/7629.jpg   \n",
              "7626  Modern country style (newly-renovated); open c...  img_train/7630.jpg   \n",
              "\n",
              "           type  price  \n",
              "0     Apartment      1  \n",
              "1     Apartment      0  \n",
              "2     Apartment      1  \n",
              "3     Apartment      1  \n",
              "4     Apartment      0  \n",
              "...         ...    ...  \n",
              "7622  Apartment      0  \n",
              "7623  Apartment      2  \n",
              "7624  Apartment      1  \n",
              "7625  Apartment      0  \n",
              "7626      House      1  \n",
              "\n",
              "[7326 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7a5177fc-a55b-4ac5-8245-e295726899a1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>summary</th>\n",
              "      <th>image</th>\n",
              "      <th>type</th>\n",
              "      <th>price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Spacious, sunny and cozy modern apartment in t...</td>\n",
              "      <td>img_train/0.jpg</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Located in one of the most vibrant and accessi...</td>\n",
              "      <td>img_train/1.jpg</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Logement coquet et douillet √† 10 minutes du ce...</td>\n",
              "      <td>img_train/2.jpg</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Beautiful and spacious (1076 sc ft, / 100 mc) ...</td>\n",
              "      <td>img_train/3.jpg</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Tr√®s grand appartement ''rustique'' et tr√®s ag...</td>\n",
              "      <td>img_train/4.jpg</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7622</th>\n",
              "      <td>Un grand logement 4 et 1/2, tout inclut, bien ...</td>\n",
              "      <td>img_train/7626.jpg</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7623</th>\n",
              "      <td>Magnificent condo directly on the river. You w...</td>\n",
              "      <td>img_train/7627.jpg</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7624</th>\n",
              "      <td>This apartment is perfect for anyone visiting ...</td>\n",
              "      <td>img_train/7628.jpg</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7625</th>\n",
              "      <td>It is a cozy ,clean ,and comfortable apartment...</td>\n",
              "      <td>img_train/7629.jpg</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7626</th>\n",
              "      <td>Modern country style (newly-renovated); open c...</td>\n",
              "      <td>img_train/7630.jpg</td>\n",
              "      <td>House</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7326 rows √ó 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7a5177fc-a55b-4ac5-8245-e295726899a1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7a5177fc-a55b-4ac5-8245-e295726899a1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7a5177fc-a55b-4ac5-8245-e295726899a1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#check the number of duplicated rows (train data)\n",
        "train.duplicated().sum()\n"
      ],
      "metadata": {
        "id": "e1KTd2qMbIHO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c7f0978-4e21-441c-dbaa-0b2bb862a8b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#check the number of duplicated rows (test data)\n",
        "test.duplicated().sum()\n"
      ],
      "metadata": {
        "id": "uYYk7OSJbaWl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77ee2c9d-b58b-450d-d7c0-96f260a1c163"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is No duplication"
      ],
      "metadata": {
        "id": "9wAfK2yN5qJU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ha4oD3LM33qi"
      },
      "source": [
        "# Text Cleaning ‚ò∫"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The summary column includes values ‚Äã‚Äãwritten in different languages, So we need to translate it into english."
      ],
      "metadata": {
        "id": "razVVzdg7YvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install googletrans==3.1.0a0"
      ],
      "metadata": {
        "id": "XjhGhaxIHfH5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a13358f-46e6-40d5-f14d-37d1aac076c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting googletrans==3.1.0a0\n",
            "  Downloading googletrans-3.1.0a0.tar.gz (19 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting httpx==0.13.3\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.9/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.3.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.9/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2022.12.7)\n",
            "Collecting hstspreload\n",
            "  Downloading hstspreload-2023.1.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting chardet==3.*\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rfc3986<2,>=1.3\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting httpcore==0.9.*\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting idna==2.*\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h2==3.*\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.10,>=0.8\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hpack<4,>=3.0\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Collecting hyperframe<6,>=5.2.0\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-3.1.0a0-py3-none-any.whl size=16368 sha256=cd159b9d29ddd4aa2049c2cf160b32d903950603172ebba52771fbedda2517fd\n",
            "  Stored in directory: /root/.cache/pip/wheels/ae/e1/6c/5137bc3f35aa130deea71575e165cc4f4f0680a88f3d90a636\n",
            "Successfully built googletrans\n",
            "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 4.0.0\n",
            "    Uninstalling chardet-4.0.0:\n",
            "      Successfully uninstalled chardet-4.0.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.4\n",
            "    Uninstalling idna-3.4:\n",
            "      Successfully uninstalled idna-3.4\n",
            "Successfully installed chardet-3.0.4 googletrans-3.1.0a0 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2023.1.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Using google translate to translate the column values\n",
        "from googletrans import Translator\n",
        "translator = Translator()\n",
        "train.summary = train.summary.apply(lambda x: translator.translate(x, dest='en').text) # translate the summary column to English (train_data)\n",
        "test.summary = test.summary.apply(lambda x: translator.translate(x, dest='en').text)  # translate the summary column to English (test_data)"
      ],
      "metadata": {
        "id": "U7hxkZasHdMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#display the train dataset\n",
        "train"
      ],
      "metadata": {
        "id": "VigXK4-8PakB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "fb07f1ce-9741-496e-9589-7583baa71af3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                summary               image  \\\n",
              "0     Spacious, sunny and cozy modern apartment in t...     img_train/0.jpg   \n",
              "1     Located in one of the most vibrant and accessi...     img_train/1.jpg   \n",
              "2     Pretty and cozy accommodation 10 minutes from ...     img_train/2.jpg   \n",
              "3     Beautiful and spacious (1076 sc ft, / 100 mc) ...     img_train/3.jpg   \n",
              "4     Very large ''rustic'' and very pleasant apartm...     img_train/4.jpg   \n",
              "...                                                 ...                 ...   \n",
              "7622  A large 4 and 1/2 apartment, all inclusive, we...  img_train/7626.jpg   \n",
              "7623  Magnificent condo directly on the river. You w...  img_train/7627.jpg   \n",
              "7624  This apartment is perfect for anyone visiting ...  img_train/7628.jpg   \n",
              "7625  It is a cozy ,clean ,and comfortable apartment...  img_train/7629.jpg   \n",
              "7626  Modern country style (newly-renovated); open c...  img_train/7630.jpg   \n",
              "\n",
              "           type  price  \n",
              "0     Apartment      1  \n",
              "1     Apartment      0  \n",
              "2     Apartment      1  \n",
              "3     Apartment      1  \n",
              "4     Apartment      0  \n",
              "...         ...    ...  \n",
              "7622  Apartment      0  \n",
              "7623  Apartment      2  \n",
              "7624  Apartment      1  \n",
              "7625  Apartment      0  \n",
              "7626      House      1  \n",
              "\n",
              "[7627 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fbddb9ba-a928-439c-97f6-e7099c37b59a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>summary</th>\n",
              "      <th>image</th>\n",
              "      <th>type</th>\n",
              "      <th>price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Spacious, sunny and cozy modern apartment in t...</td>\n",
              "      <td>img_train/0.jpg</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Located in one of the most vibrant and accessi...</td>\n",
              "      <td>img_train/1.jpg</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Pretty and cozy accommodation 10 minutes from ...</td>\n",
              "      <td>img_train/2.jpg</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Beautiful and spacious (1076 sc ft, / 100 mc) ...</td>\n",
              "      <td>img_train/3.jpg</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Very large ''rustic'' and very pleasant apartm...</td>\n",
              "      <td>img_train/4.jpg</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7622</th>\n",
              "      <td>A large 4 and 1/2 apartment, all inclusive, we...</td>\n",
              "      <td>img_train/7626.jpg</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7623</th>\n",
              "      <td>Magnificent condo directly on the river. You w...</td>\n",
              "      <td>img_train/7627.jpg</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7624</th>\n",
              "      <td>This apartment is perfect for anyone visiting ...</td>\n",
              "      <td>img_train/7628.jpg</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7625</th>\n",
              "      <td>It is a cozy ,clean ,and comfortable apartment...</td>\n",
              "      <td>img_train/7629.jpg</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7626</th>\n",
              "      <td>Modern country style (newly-renovated); open c...</td>\n",
              "      <td>img_train/7630.jpg</td>\n",
              "      <td>House</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7627 rows √ó 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fbddb9ba-a928-439c-97f6-e7099c37b59a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fbddb9ba-a928-439c-97f6-e7099c37b59a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fbddb9ba-a928-439c-97f6-e7099c37b59a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see a summary column has translate into English üòÄ"
      ],
      "metadata": {
        "id": "38weK8Bib4tq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For dealing with text related tasks, we will be using nltk. The terrific scikit-learn library will be used to handle tasks related to machine learning.\n"
      ],
      "metadata": {
        "id": "FYsEL2BtLTMs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "vHHl4ip4tCx5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c863103-9e3f-4124-d682-7a66b5e5514b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `clean_text` function takes a string input and applies a bunch of manipulations to it (described in the code)."
      ],
      "metadata": {
        "id": "Yk4XEGSiQseR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words(\"english\"))\n",
        "def clean_text(text, for_embedding=False):\n",
        "    \"\"\" steps:\n",
        "        - remove any html tags (< /br> often found)\n",
        "        - remove single letter chars\n",
        "        - convert all whitespaces (tabs etc.) to single wspace\n",
        "        if not for embedding (but e.g. tdf-idf):\n",
        "        - all lowercase\n",
        "        - remove stopwords, punctuation and stemm\n",
        "    \"\"\"\n",
        "    # match one or more white sepace\n",
        "    RE_WSPACE = re.compile(r\"\\s+\", re.IGNORECASE)\n",
        "    # match <any num of words>\n",
        "    RE_TAGS = re.compile(r\"<.*?>\")\n",
        "    # match any word with word boundary\n",
        "    RE_SINGLECHAR = re.compile(r\"\\b^[^A-Za-z√Ä-≈æ0-9]+\\b\", re.IGNORECASE)\n",
        "    if for_embedding:\n",
        "        # Keep punctuation\n",
        "        # match any word and any punctuation with word boundary.\n",
        "        RE_SINGLECHAR = re.compile(r\"\\b[A-Za-z√Ä-≈æ,.!?]\\b\", re.IGNORECASE)\n",
        "\n",
        "    #remove <any num of words>\n",
        "    text = re.sub(RE_TAGS, \" \", text)\n",
        "    #remove any word with word boundary\n",
        "    text = re.sub(RE_SINGLECHAR, \" \", text)\n",
        "    #remove one or more white sepace\n",
        "    text = re.sub(RE_WSPACE, \" \", text)\n",
        "\n",
        "    \n",
        "    word_tokens = word_tokenize(text)\n",
        "\n",
        "    return word_tokens"
      ],
      "metadata": {
        "id": "6QCjBALptIig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `lemma_clean` function"
      ],
      "metadata": {
        "id": "EYKtgufASYc0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lemma_clean(text, for_embedding=False):\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  word_tokens = clean_text(text, for_embedding)\n",
        "  ''' steps:\n",
        "        if not for embedding (but e.g. tdf-idf):\n",
        "        - all lowercase\n",
        "        - remove stopwords, punctuation and lemmatize\n",
        "    '''\n",
        "\n",
        "  if for_embedding:\n",
        "    # no lemmatization, lowering and punctuation / stop words removal\n",
        "    words_filtered = word_tokens\n",
        "  else:\n",
        "    words_tokens_lower = [word.lower() for word in word_tokens]\n",
        "\n",
        "    words_filtered = [lemmatizer.lemmatize(word) for word in words_tokens_lower if word not in stop_words]\n",
        "\n",
        "    text_clean = \" \".join(words_filtered)\n",
        "    return text_clean"
      ],
      "metadata": {
        "id": "AdqSiUu9t_RC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we can improve model performance again by increasing the number of relevant data points.\n",
        "Let's apply this to our data:"
      ],
      "metadata": {
        "id": "lYWTFhC0Sed6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train[\"summary\"] = train.loc[train[\"summary\"].str.len() > 20, \"summary\"]\n",
        "test[\"summary\"] = test.loc[test[\"summary\"].str.len() > 20, \"summary\"]"
      ],
      "metadata": {
        "id": "1ap4o5bPw5F4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing Summary Column with Lemmatizing"
      ],
      "metadata": {
        "id": "wuI56reIQkIa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean text (train set)\n",
        "#clean text with lemmatizing each word in the text \n",
        "#(lemmatizing will remove any word ending with take in consideration the meaning of the word).\n",
        "train['summary']=train['summary'].map(lambda x: lemma_clean(x, for_embedding=False) if isinstance(x, str) else x)"
      ],
      "metadata": {
        "id": "pWWtHyzi4TIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean text (test set)\n",
        "#clean text with lemmatizing each word in the text \n",
        "#(lemmatizing will remove any word ending with take in consideration the meaning of the word).\n",
        "test['summary']=test['summary'].map(lambda x: lemma_clean(x, for_embedding=False) if isinstance(x, str) else x)"
      ],
      "metadata": {
        "id": "gQipTHl-RNn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's encode the prediction labels and calculate the total number \n",
        "of unique labels. After, lets split the dataset into training set and testing set."
      ],
      "metadata": {
        "id": "_ozA65iIccQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#convert labels to categories\n",
        "train['type'] = train.type.astype('category').cat.codes\n",
        "len_type = len(train.type.unique())\n",
        "len_price = len(train.price.unique())\n",
        "# get the total number of unique outputs (later used for prediction)\n",
        "# split data (training/validation set)\n",
        "X, Y = train_test_split(train, test_size=0.2, random_state=20)"
      ],
      "metadata": {
        "id": "cSh43dHyRTuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading summary: (force convert some of the non-string cell to string)\n",
        "x_train_text = train.summary.astype('str')"
      ],
      "metadata": {
        "id": "6O3uD9GCcE7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing üßπ\n",
        "\n",
        "We have image and text data. \n",
        "- Image data: resize each image and put its value in numpy array.\n",
        "- Text data: tokenization and converting to integer IDs.\n"
      ],
      "metadata": {
        "id": "FUoMACSf_MRK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# preprocessing on image column"
      ],
      "metadata": {
        "id": "DrmuVWHBT9t6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we need to resize values in the image column "
      ],
      "metadata": {
        "id": "ZLVq4Avy1TlX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess image data\n",
        "#function to preprocss the image\n",
        "# resize the image\n",
        "\n",
        "import os\n",
        "from PIL import Image\n",
        "def load_image(file):\n",
        "    try:\n",
        "        image = Image.open(\n",
        "            file\n",
        "        ).convert('LA').resize((64, 64)) #resize image\n",
        "        arr = np.array(image)\n",
        "    except:\n",
        "        #if the file doesn't exsit store array of zeros\n",
        "        arr = np.zeros((64, 64, 2))\n",
        "    return arr"
      ],
      "metadata": {
        "id": "bIt-9QF5RXXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#display image column \n",
        "train['image']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnNiVb3KRjQg",
        "outputId": "1778e50a-b258-486c-b558-f12e0963e60d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0          img_train/0.jpg\n",
              "1          img_train/1.jpg\n",
              "2          img_train/2.jpg\n",
              "3          img_train/3.jpg\n",
              "4          img_train/4.jpg\n",
              "               ...        \n",
              "7622    img_train/7626.jpg\n",
              "7623    img_train/7627.jpg\n",
              "7624    img_train/7628.jpg\n",
              "7625    img_train/7629.jpg\n",
              "7626    img_train/7630.jpg\n",
              "Name: image, Length: 7627, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loading images:\n",
        "x_train_image = np.array([load_image('/content/drive/MyDrive/copy-of-cisc-873-dm-w23-a4.zip (Unzipped Files)/a4/'+str(i)) for i in tqdm(train['image'])])\n",
        "x_test_image = np.array([load_image('/content/drive/MyDrive/copy-of-cisc-873-dm-w23-a4.zip (Unzipped Files)/a4/'+str(i)) for i in tqdm(test['image'])])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "9c7d40c561824e9ca10a7ac96bf5d712",
            "04040bf4af6748169f661887bf780094",
            "7f05579a3c664b52900084ecec591c39",
            "fa08d086a33f4957b7e50b189ea2c41f",
            "324f6f92b60a4259a5763a670cf52280",
            "f1c617ce9bc34098be63688060bfd740",
            "00b00d030ed94195a52d9177364ed7bb",
            "3382a412020e4a89beaecded09170935",
            "8c0a683b1a084cb7b3d593e0aef6377a",
            "0e84099137d34a6d863e82cf68c5b1ad",
            "dbb591f7b63f4b2395a91a5598bcb0f4",
            "375415aa9072474aad0c166993c87605",
            "06f2392f856d44a48def359c260ea50d",
            "c62ae040005e432b826344db98130b1b",
            "406ce9e619f640e68e6249e41207bce3",
            "361d310eb58a4faf9303ad7660b93e9f",
            "9d105eda59a946b392df43b288fb6fbe",
            "6c7f443cd60e4c4e947e71d65876917f",
            "c85eb13c4a824a3a83886356280147cc",
            "ed2886d0ce6b4bafa7a6f16a43551c95",
            "1c4cbe1e9f494e0a998556c131d7b46b",
            "fc6e8c9f75e44cc5a3bed80f94a05815"
          ]
        },
        "outputId": "f83d51bf-7f88-41ea-97b7-89524b245396",
        "id": "w8d8Ib0HSd1h"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/7627 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9c7d40c561824e9ca10a7ac96bf5d712"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/7360 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "375415aa9072474aad0c166993c87605"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Definig the labels\n",
        "y_train_price = train['price'] # get price\n",
        "y_train_type = train['type'] # get type \n"
      ],
      "metadata": {
        "id": "hObPvdxdRXXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check image loading\n",
        "plt.imshow(x_train_image[50, :, :, 0])\n",
        "x_train_image.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "outputId": "682d9632-01c4-4134-812d-e5b70c4c54e6",
        "id": "0-JGTrHuRXXC"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7627, 64, 64, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess text data\n",
        "#tokenize each text and then give token a unique id.\n",
        "\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from pprint import pprint\n",
        "\n",
        "vocab_size = 40000\n",
        "max_len = 100\n",
        "\n",
        "\n",
        "# build vocabulary from training set\n",
        "tokenizer = Tokenizer(num_words=vocab_size)\n",
        "tokenizer.fit_on_texts(x_train_text)\n",
        "\n",
        "\n",
        "def _preprocess(list_of_text):\n",
        "    return pad_sequences(\n",
        "        tokenizer.texts_to_sequences(list_of_text),\n",
        "        maxlen=max_len,\n",
        "        padding='post',\n",
        "    )\n",
        "    \n",
        "\n",
        "# padding is done inside: \n",
        "x_train_text_id = _preprocess(x_train_text)\n",
        "\n",
        "print(x_train_text_id.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmLXR_KubLD_",
        "outputId": "d39310b5-dcd7-4200-903a-64851d5eca5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7627, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fxoXMDER1Zm"
      },
      "outputs": [],
      "source": [
        "# loading summary: (force convert some of the non-string cell to string)\n",
        "x_test_text = _preprocess(test.summary.astype('str'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5E5zYSiR1Zm",
        "outputId": "9d905834-1e59-499d-9a9e-266404472af8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['spacious sunny and cozy modern apartment in the heart of montreal this 3 '\n",
            " 'bedroom centrally located in the very popular plateau mont royal '\n",
            " 'neighborhood in the middle of prince arthur pedestrian only street close to '\n",
            " 'all amenities restaurants coffee house bars clubs shopping universities '\n",
            " 'subway stations experience montreal like a real local resident be in the '\n",
            " 'heart of the action grand prix week grill saint laurent festival mural 2019 '\n",
            " 'and so much more',\n",
            " 'located in one of the most vibrant and accessible locations of downtown '\n",
            " 'montreal this one bedroom condo will not only impress you but leave you with '\n",
            " 'one of the most memorable experiences it is walking distance of the popular '\n",
            " 'sainte catherine street the bell center the old port lachine canal '\n",
            " 'bonaventure metro and much much more',\n",
            " 'pretty and cozy accommodation 10 minutes from downtown montreal grocery '\n",
            " 'store pharmacy saq restaurants and public transport nearby two closed '\n",
            " 'bedrooms that can accommodate 4 adults',\n",
            " 'beautiful and spacious 1076 sc ft 100 mc condo on the 1th floor in the west '\n",
            " 'island of montreal located in a quiet residential area near a number of '\n",
            " 'superb green spaces 3 min car from highways 13 40 7 min from le march√© de '\n",
            " \"l'ouest 15 min from the airport 6 min walk from sunnybrooke train station \"\n",
            " 'that goes downtown mtl in 20 min and is close to the beautiful nature park '\n",
            " 'bois de liesse walk and bike paths in summer and snowshoe and cross country '\n",
            " 'skiing paths in winter',\n",
            " \"very large ''rustic'' and very pleasant apartment for rent in a nice \"\n",
            " 'neighborhood of montreal a large bedroom in the basement and on the ground '\n",
            " 'floor there is a bedroom a large kitchen dining room and a large double '\n",
            " 'living room the kitchen offers many machines such as a blender a food '\n",
            " 'processor a coffee maker a panini machine a waffle machine very close to the '\n",
            " 'riverfront and downtown verdun and montreal on foot by metro bike or bus']\n"
          ]
        }
      ],
      "source": [
        "# we can use the tokenizer to convert IDs to words.\n",
        "pprint(tokenizer.sequences_to_texts(x_train_text_id[:5]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75tmwBWAR1Zm",
        "outputId": "7f777963-64bf-4371-9ad5-12ae6197aeee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total words in the dictionary: 40000\n"
          ]
        }
      ],
      "source": [
        "print('total words in the dictionary:', tokenizer.num_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building a Deep learing models"
      ],
      "metadata": {
        "id": "l24YEFKBA00d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#First Trail"
      ],
      "metadata": {
        "id": "7YJ03eMkgVRn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multi-modality learning with Multi-objective learning**\n",
        "\n",
        "\n",
        "*   Multi-modality learning with multi-objective learning can be more versatile and comprehensive, as it can combine the benefits of both approaches and handle complex problems that involve multiple input data sources and multiple output tasks. However, it may also face some challenges, such as designing a suitable model architecture that can fuse and optimize multiple modalities and tasks, tuning the hyperparameters and loss functions for each modality and task, and evaluating the performance and trade-offs of the model.\n",
        "*   It's take summary and image as an input and predict price and type as output.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lsWOFWGoT1Dv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Conv2d layer.\n",
        "\n",
        "This layer creates a convolution kernel that is convolved with the layer input to produce a tensor of outputs.\n"
      ],
      "metadata": {
        "id": "Fbdo45QNgXwX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a Conv2D layer can be used to process the image data, but not the text data. Text data is usually represented as a sequence of tokens or characters, which does not have a spatial structure that can be exploited by a Conv2D layer. A better choice for text data would be an embedding layer, which can map the tokens or characters to a high-dimensional vector space, followed by a recurrent or attention layer, which can capture the sequential dependencies and semantics of the text."
      ],
      "metadata": {
        "id": "U639vKgm6_tH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QkkRmqYR1Zm",
        "outputId": "909132e7-f3b1-4076-b828-d4ae79256959"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 64, 64, 2)]  0           []                               \n",
            "                                                                                                  \n",
            " input_1 (InputLayer)           [(None, 100)]        0           []                               \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 49, 49, 32)   16416       ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 100, 100)     4000000     ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " max_pooling2d (MaxPooling2D)   (None, 3, 3, 32)     0           ['conv2d[0][0]']                 \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean (TFOpLambd  (None, 100)         0           ['embedding[0][0]']              \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 288)          0           ['max_pooling2d[0][0]']          \n",
            "                                                                                                  \n",
            " tf.concat (TFOpLambda)         (None, 388)          0           ['tf.math.reduce_mean[0][0]',    \n",
            "                                                                  'flatten[0][0]']                \n",
            "                                                                                                  \n",
            " price (Dense)                  (None, 3)            1167        ['tf.concat[0][0]']              \n",
            "                                                                                                  \n",
            " type (Dense)                   (None, 24)           9336        ['tf.concat[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4,026,919\n",
            "Trainable params: 4,026,919\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import collections\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Conv2D, Flatten, Dense, MaxPool2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "\n",
        "# here we have two inputs. one for image and the other for text.\n",
        "in_text = keras.Input(batch_shape=(None, max_len))\n",
        "in_image = keras.Input(batch_shape=(None, 64, 64, 2))\n",
        "\n",
        "# text part\n",
        "# simple average of embedding. you can change it to anything else as needed\n",
        "embedded = keras.layers.Embedding(tokenizer.num_words, 100)(in_text)\n",
        "averaged = tf.reduce_mean(embedded, axis=1)\n",
        "\n",
        "\n",
        "# image part \n",
        "# simple conv2d. you can change it to anything else as needed\n",
        "cov = Conv2D(32, (16, 16))(in_image)\n",
        "pl = MaxPool2D((16, 16))(cov)\n",
        "flattened = Flatten()(pl)\n",
        "\n",
        "\n",
        "# fusion - combinig both\n",
        "fused = tf.concat([averaged, flattened], axis=-1)\n",
        "\n",
        "# multi-task learning (each is a multi-class classification)\n",
        "# one dense layer for each task\n",
        "p_type = Dense(len_type, activation='softmax', name='type')(fused)\n",
        "p_price = Dense(len_price, activation='softmax', name='price')(fused)\n",
        "\n",
        "\n",
        "# define model input/output using keys.\n",
        "model = keras.Model(\n",
        "    inputs={\n",
        "        'summary': in_text,\n",
        "        'image': in_image\n",
        "    },\n",
        "    outputs={\n",
        "        'type': p_type,\n",
        "        'price': p_price,\n",
        "    },\n",
        ")\n",
        "\n",
        "\n",
        "# compile model with optimizer, loss values for each task, loss \n",
        "# weights for each task.\n",
        "model.compile(\n",
        "    optimizer=Adam(),\n",
        "    loss={\n",
        "        'type': 'sparse_categorical_crossentropy',\n",
        "        'price': 'sparse_categorical_crossentropy',\n",
        "    },\n",
        "    loss_weights={\n",
        "        'type': 0.5,\n",
        "        'price': 0.5,       \n",
        "    },\n",
        "    metrics={\n",
        "        'type': ['SparseCategoricalAccuracy'],\n",
        "        'price': ['SparseCategoricalAccuracy'],\n",
        "    },\n",
        ")\n",
        "\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training\n",
        "\n",
        "Based on the training/validation performance, you can adjust the epochs to be trained. Early stoping is watching the validation loss on genre prediction (assuming that it is the main task we would like to perform)"
      ],
      "metadata": {
        "id": "UB-r6sfjBYh9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyVFF4LTR1Zm",
        "outputId": "6f19208d-38f2-4a2b-bed1-3f64c3b6ed93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 21.9245 - price_loss: 20.4631 - type_loss: 23.3860 - price_sparse_categorical_accuracy: 0.4883 - type_sparse_categorical_accuracy: 0.5797"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 50s 99ms/step - loss: 21.9245 - price_loss: 20.4631 - type_loss: 23.3860 - price_sparse_categorical_accuracy: 0.4883 - type_sparse_categorical_accuracy: 0.5797 - val_loss: 6.9047 - val_price_loss: 4.5866 - val_type_loss: 9.2229 - val_price_sparse_categorical_accuracy: 0.4725 - val_type_sparse_categorical_accuracy: 0.6913\n",
            "Epoch 2/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 6.7772 - price_loss: 5.0646 - type_loss: 8.4897 - price_sparse_categorical_accuracy: 0.5150 - type_sparse_categorical_accuracy: 0.5843"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 11s 28ms/step - loss: 6.7772 - price_loss: 5.0646 - type_loss: 8.4897 - price_sparse_categorical_accuracy: 0.5150 - type_sparse_categorical_accuracy: 0.5843 - val_loss: 34.3715 - val_price_loss: 40.1444 - val_type_loss: 28.5986 - val_price_sparse_categorical_accuracy: 0.1239 - val_type_sparse_categorical_accuracy: 0.7647\n",
            "Epoch 3/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 9.0852 - price_loss: 6.2913 - type_loss: 11.8792 - price_sparse_categorical_accuracy: 0.5099 - type_sparse_categorical_accuracy: 0.5873"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 5s 14ms/step - loss: 9.0852 - price_loss: 6.2913 - type_loss: 11.8792 - price_sparse_categorical_accuracy: 0.5099 - type_sparse_categorical_accuracy: 0.5873 - val_loss: 5.8661 - val_price_loss: 4.5204 - val_type_loss: 7.2118 - val_price_sparse_categorical_accuracy: 0.5131 - val_type_sparse_categorical_accuracy: 0.6887\n",
            "Epoch 4/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 5.7391 - price_loss: 4.1716 - type_loss: 7.3066 - price_sparse_categorical_accuracy: 0.5483 - type_sparse_categorical_accuracy: 0.5911"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 4s 11ms/step - loss: 5.7391 - price_loss: 4.1716 - type_loss: 7.3066 - price_sparse_categorical_accuracy: 0.5483 - type_sparse_categorical_accuracy: 0.5911 - val_loss: 5.0698 - val_price_loss: 3.7275 - val_type_loss: 6.4122 - val_price_sparse_categorical_accuracy: 0.4961 - val_type_sparse_categorical_accuracy: 0.4364\n",
            "Epoch 5/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 7.0335 - price_loss: 5.3163 - type_loss: 8.7506 - price_sparse_categorical_accuracy: 0.5512 - type_sparse_categorical_accuracy: 0.6019"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 6s 17ms/step - loss: 7.0335 - price_loss: 5.3163 - type_loss: 8.7506 - price_sparse_categorical_accuracy: 0.5512 - type_sparse_categorical_accuracy: 0.6019 - val_loss: 14.1170 - val_price_loss: 6.5978 - val_type_loss: 21.6361 - val_price_sparse_categorical_accuracy: 0.6003 - val_type_sparse_categorical_accuracy: 0.3211\n",
            "Epoch 6/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 6.6453 - price_loss: 4.4469 - type_loss: 8.8436 - price_sparse_categorical_accuracy: 0.5684 - type_sparse_categorical_accuracy: 0.6060"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 3s 9ms/step - loss: 6.6453 - price_loss: 4.4469 - type_loss: 8.8436 - price_sparse_categorical_accuracy: 0.5684 - type_sparse_categorical_accuracy: 0.6060 - val_loss: 6.7575 - val_price_loss: 4.4133 - val_type_loss: 9.1018 - val_price_sparse_categorical_accuracy: 0.6081 - val_type_sparse_categorical_accuracy: 0.3283\n",
            "Epoch 7/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 16.2338 - price_loss: 10.2472 - type_loss: 22.2203 - price_sparse_categorical_accuracy: 0.5465 - type_sparse_categorical_accuracy: 0.5932"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 4s 9ms/step - loss: 16.2338 - price_loss: 10.2472 - type_loss: 22.2203 - price_sparse_categorical_accuracy: 0.5465 - type_sparse_categorical_accuracy: 0.5932 - val_loss: 34.1651 - val_price_loss: 21.0122 - val_type_loss: 47.3180 - val_price_sparse_categorical_accuracy: 0.6258 - val_type_sparse_categorical_accuracy: 0.4535\n",
            "Epoch 8/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 7.7085 - price_loss: 4.5116 - type_loss: 10.9055 - price_sparse_categorical_accuracy: 0.5917 - type_sparse_categorical_accuracy: 0.6265"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 5s 14ms/step - loss: 7.7085 - price_loss: 4.5116 - type_loss: 10.9055 - price_sparse_categorical_accuracy: 0.5917 - type_sparse_categorical_accuracy: 0.6265 - val_loss: 5.2347 - val_price_loss: 3.9136 - val_type_loss: 6.5557 - val_price_sparse_categorical_accuracy: 0.5806 - val_type_sparse_categorical_accuracy: 0.5577\n",
            "Epoch 9/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 4.9433 - price_loss: 3.3275 - type_loss: 6.5591 - price_sparse_categorical_accuracy: 0.6160 - type_sparse_categorical_accuracy: 0.6471"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 3s 8ms/step - loss: 4.9433 - price_loss: 3.3275 - type_loss: 6.5591 - price_sparse_categorical_accuracy: 0.6160 - type_sparse_categorical_accuracy: 0.6471 - val_loss: 4.0304 - val_price_loss: 3.2177 - val_type_loss: 4.8431 - val_price_sparse_categorical_accuracy: 0.6271 - val_type_sparse_categorical_accuracy: 0.6894\n",
            "Epoch 10/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 10.8519 - price_loss: 7.0583 - type_loss: 14.6455 - price_sparse_categorical_accuracy: 0.6127 - type_sparse_categorical_accuracy: 0.6324"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 4s 10ms/step - loss: 10.8519 - price_loss: 7.0583 - type_loss: 14.6455 - price_sparse_categorical_accuracy: 0.6127 - type_sparse_categorical_accuracy: 0.6324 - val_loss: 6.3064 - val_price_loss: 4.6000 - val_type_loss: 8.0128 - val_price_sparse_categorical_accuracy: 0.4738 - val_type_sparse_categorical_accuracy: 0.7104\n",
            "Epoch 11/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 6.2337 - price_loss: 4.0069 - type_loss: 8.4606 - price_sparse_categorical_accuracy: 0.6284 - type_sparse_categorical_accuracy: 0.6469"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 4s 9ms/step - loss: 6.2337 - price_loss: 4.0069 - type_loss: 8.4606 - price_sparse_categorical_accuracy: 0.6284 - type_sparse_categorical_accuracy: 0.6469 - val_loss: 7.8470 - val_price_loss: 4.7143 - val_type_loss: 10.9797 - val_price_sparse_categorical_accuracy: 0.4830 - val_type_sparse_categorical_accuracy: 0.6763\n",
            "Epoch 12/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 6.6637 - price_loss: 3.6491 - type_loss: 9.6783 - price_sparse_categorical_accuracy: 0.6348 - type_sparse_categorical_accuracy: 0.6455"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 4s 11ms/step - loss: 6.6637 - price_loss: 3.6491 - type_loss: 9.6783 - price_sparse_categorical_accuracy: 0.6348 - type_sparse_categorical_accuracy: 0.6455 - val_loss: 6.2274 - val_price_loss: 4.2807 - val_type_loss: 8.1740 - val_price_sparse_categorical_accuracy: 0.4705 - val_type_sparse_categorical_accuracy: 0.7543\n",
            "Epoch 13/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 6.2861 - price_loss: 3.9268 - type_loss: 8.6453 - price_sparse_categorical_accuracy: 0.6419 - type_sparse_categorical_accuracy: 0.6569"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 3s 9ms/step - loss: 6.2861 - price_loss: 3.9268 - type_loss: 8.6453 - price_sparse_categorical_accuracy: 0.6419 - type_sparse_categorical_accuracy: 0.6569 - val_loss: 4.2370 - val_price_loss: 3.0278 - val_type_loss: 5.4461 - val_price_sparse_categorical_accuracy: 0.5747 - val_type_sparse_categorical_accuracy: 0.5911\n",
            "Epoch 14/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 18.4082 - price_loss: 10.8825 - type_loss: 25.9338 - price_sparse_categorical_accuracy: 0.6051 - type_sparse_categorical_accuracy: 0.6291"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 4s 9ms/step - loss: 18.4082 - price_loss: 10.8825 - type_loss: 25.9338 - price_sparse_categorical_accuracy: 0.6051 - type_sparse_categorical_accuracy: 0.6291 - val_loss: 7.4958 - val_price_loss: 5.5949 - val_type_loss: 9.3967 - val_price_sparse_categorical_accuracy: 0.4069 - val_type_sparse_categorical_accuracy: 0.6075\n",
            "Epoch 15/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 7.5350 - price_loss: 3.9240 - type_loss: 11.1460 - price_sparse_categorical_accuracy: 0.6528 - type_sparse_categorical_accuracy: 0.6656"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 3s 9ms/step - loss: 7.5350 - price_loss: 3.9240 - type_loss: 11.1460 - price_sparse_categorical_accuracy: 0.6528 - type_sparse_categorical_accuracy: 0.6656 - val_loss: 4.6241 - val_price_loss: 2.9987 - val_type_loss: 6.2494 - val_price_sparse_categorical_accuracy: 0.6429 - val_type_sparse_categorical_accuracy: 0.7353\n",
            "Epoch 16/20\n",
            "376/382 [============================>.] - ETA: 0s - loss: 5.7784 - price_loss: 3.6127 - type_loss: 7.9440 - price_sparse_categorical_accuracy: 0.6769 - type_sparse_categorical_accuracy: 0.6755"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 3s 9ms/step - loss: 5.7758 - price_loss: 3.6059 - type_loss: 7.9458 - price_sparse_categorical_accuracy: 0.6764 - type_sparse_categorical_accuracy: 0.6766 - val_loss: 9.4011 - val_price_loss: 4.5577 - val_type_loss: 14.2446 - val_price_sparse_categorical_accuracy: 0.5138 - val_type_sparse_categorical_accuracy: 0.3761\n",
            "Epoch 17/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 3.4117 - price_loss: 2.1391 - type_loss: 4.6843 - price_sparse_categorical_accuracy: 0.7081 - type_sparse_categorical_accuracy: 0.7061"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 3s 8ms/step - loss: 3.4117 - price_loss: 2.1391 - type_loss: 4.6843 - price_sparse_categorical_accuracy: 0.7081 - type_sparse_categorical_accuracy: 0.7061 - val_loss: 4.0135 - val_price_loss: 2.7055 - val_type_loss: 5.3216 - val_price_sparse_categorical_accuracy: 0.6363 - val_type_sparse_categorical_accuracy: 0.6409\n",
            "Epoch 18/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 3.1062 - price_loss: 1.9171 - type_loss: 4.2953 - price_sparse_categorical_accuracy: 0.7140 - type_sparse_categorical_accuracy: 0.7037"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 3s 8ms/step - loss: 3.1062 - price_loss: 1.9171 - type_loss: 4.2953 - price_sparse_categorical_accuracy: 0.7140 - type_sparse_categorical_accuracy: 0.7037 - val_loss: 4.1969 - val_price_loss: 2.6886 - val_type_loss: 5.7052 - val_price_sparse_categorical_accuracy: 0.6127 - val_type_sparse_categorical_accuracy: 0.6940\n",
            "Epoch 19/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 3.2896 - price_loss: 2.1716 - type_loss: 4.4077 - price_sparse_categorical_accuracy: 0.7017 - type_sparse_categorical_accuracy: 0.6945"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 3s 7ms/step - loss: 3.2896 - price_loss: 2.1716 - type_loss: 4.4077 - price_sparse_categorical_accuracy: 0.7017 - type_sparse_categorical_accuracy: 0.6945 - val_loss: 4.0464 - val_price_loss: 2.6506 - val_type_loss: 5.4422 - val_price_sparse_categorical_accuracy: 0.6402 - val_type_sparse_categorical_accuracy: 0.6232\n",
            "Epoch 20/20\n",
            "374/382 [============================>.] - ETA: 0s - loss: 17.3401 - price_loss: 11.7398 - type_loss: 22.9404 - price_sparse_categorical_accuracy: 0.6551 - type_sparse_categorical_accuracy: 0.6653"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 5s 12ms/step - loss: 17.2155 - price_loss: 11.5540 - type_loss: 22.8770 - price_sparse_categorical_accuracy: 0.6563 - type_sparse_categorical_accuracy: 0.6643 - val_loss: 20.8148 - val_price_loss: 5.1543 - val_type_loss: 36.4753 - val_price_sparse_categorical_accuracy: 0.6494 - val_type_sparse_categorical_accuracy: 0.3899\n"
          ]
        }
      ],
      "source": [
        "\n",
        "history = model.fit(\n",
        "    x={\n",
        "        'summary': x_train_text_id,\n",
        "        'image': x_train_image\n",
        "    },\n",
        "    y={\n",
        "        'type': y_train_type,\n",
        "        'price': y_train_price,\n",
        "    },\n",
        "    epochs=20,\n",
        "    batch_size=16,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_genre_loss', patience=5, )\n",
        "    ],\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predition\n",
        "\n",
        "We can use the model to predict the testing samples."
      ],
      "metadata": {
        "id": "FdVbNitNBynm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZoqKB--AZYgc",
        "outputId": "c5bc65f8-f13d-4387-8d27-10b18bdc2119"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "230/230 [==============================] - 1s 2ms/step\n",
            "[[9.9985480e-01 1.4517405e-04 1.1646709e-08]\n",
            " [9.9999976e-01 1.8438443e-07 1.9841703e-20]\n",
            " [9.9996817e-01 3.1853302e-05 1.3526767e-21]\n",
            " ...\n",
            " [1.0000000e+00 4.3763248e-12 1.9498581e-27]\n",
            " [1.0000000e+00 8.4636413e-09 5.4708873e-21]\n",
            " [1.0000000e+00 3.7933781e-09 4.9399984e-25]]\n",
            "[0 0 0 ... 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "# we can do prediction on training set\n",
        "y_predict = model.predict(\n",
        "    {\n",
        "        'summary': x_test_text,\n",
        "        'image': x_test_image\n",
        "    }\n",
        ")\n",
        "\n",
        "\n",
        "# probabilities\n",
        "price_predicted = y_predict['price']\n",
        "print(price_predicted)\n",
        "\n",
        "# categories\n",
        "price_category_predicted = np.argmax(price_predicted, axis=1)\n",
        "print(price_category_predicted)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHUXFXX5R1Zm"
      },
      "outputs": [],
      "source": [
        "#  (if for kaggle competition and it is about genre prediction)\n",
        "pd.DataFrame(\n",
        "    {'id': test.id,\n",
        "     'price': price_category_predicted}\n",
        ").to_csv('sample_submission1.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This Trail Get Score on Kaggle (0.62826)\n",
        "\n",
        "The Score without lemmatization (0.64782)"
      ],
      "metadata": {
        "id": "woTMxrQdv3-Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model didn't overfit on the training dataset"
      ],
      "metadata": {
        "id": "i3naNJDsUL9x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Secound Trail "
      ],
      "metadata": {
        "id": "O6_i4pxK0081"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multi-modality learning with Multi-objective learning**\n",
        "\n",
        "\n",
        "*   Multi-modality learning with multi-objective learning can be more versatile and comprehensive, as it can combine the benefits of both approaches and handle complex problems that involve multiple input data sources and multiple output tasks. However, it may also face some challenges, such as designing a suitable model architecture that can fuse and optimize multiple modalities and tasks, tuning the hyperparameters and loss functions for each modality and task, and evaluating the performance and trade-offs of the model.\n",
        "*   It's take summary and image as an input and predict price and type as output.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JVDRczZPBKH4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using LSTM and Droupout layer"
      ],
      "metadata": {
        "id": "Kn_8chij1jjj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM stands for Long-Short Term Memory. LSTM is a type of recurrent neural network but is better than traditional recurrent neural networks in terms of memory. Having a good hold over memorizing certain patterns LSTMs perform fairly better. As with every other NN, LSTM can have multiple hidden layers and as it passes through every layer, the relevant information is kept and all the irrelevant information gets discarded in every single cell.\n",
        "\n",
        "LSTM has 3 main gates.\n",
        "\n",
        "1. FORGET Gate\n",
        "2. INPUT Gate\n",
        "3. OUTPUT Gate"
      ],
      "metadata": {
        "id": "GUtyhFFX1f-L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTMs efficiently improves performance by memorizing the relevant information that is important and finds the pattern"
      ],
      "metadata": {
        "id": "x-7hol3g2AKj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dropout is a regularization technique that prevents neural networks from overfitting."
      ],
      "metadata": {
        "id": "VegMNkcm4jjD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I think this model will be better than the pervious model"
      ],
      "metadata": {
        "id": "tZFQHPNGU0_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import LSTM\n",
        "from keras.layers.core import SpatialDropout1D,Dropout\n",
        "from keras import regularizers\n",
        "\n",
        "\n",
        "\n",
        "# text part\n",
        "# simple average of embedding. you can change it to anything else as needed\n",
        "embedded = keras.layers.Embedding(tokenizer.num_words, 250)(in_text)\n",
        "spatial=SpatialDropout1D(0.2)(embedded)\n",
        "lstm=LSTM(32,dropout=0.2,return_sequences=True)(spatial)\n",
        "averaged = tf.reduce_mean(lstm, axis=1)\n",
        "\n",
        "#images part\n",
        "cov = Conv2D(32, (8, 8),activation='relu',kernel_regularizer=regularizers.l2(0.01),kernel_initializer='he_uniform')(in_image)\n",
        "pl = MaxPool2D(pool_size=(8, 8))(cov)#,strides=(1,1),padding='same')(cov) \n",
        "d1=Dropout(0.25)(pl)\n",
        "cov2 = Conv2D(32, (4, 4),kernel_regularizer=regularizers.l2(0.01),activation='relu')(d1)\n",
        "p2 = MaxPool2D((4, 4))(cov2)#,strides=(1,1),padding='same')(cov2)\n",
        "d2=Dropout(0.25)(p2)\n",
        "flattened = Flatten()(d2)\n",
        "\n",
        "\n",
        "# fusion - combinig both\n",
        "fused = tf.concat([averaged, flattened], axis=-1)\n",
        "\n",
        "# multi-task learning (each is a multi-class classification)\n",
        "# one dense layer for each task\n",
        "p_type = Dense(len_type, activation='softmax', name='type')(fused)\n",
        "p_price = Dense(len_price, activation='softmax', name='price')(fused)\n",
        "\n",
        "\n",
        "# define model input/output using keys.\n",
        "model2 = keras.Model(\n",
        "    inputs={\n",
        "        'summary': in_text,\n",
        "            'image': in_image\n",
        "            },\n",
        "    outputs={\n",
        "        'type': p_type,\n",
        "        'price': p_price,\n",
        "        },\n",
        "    )\n",
        "\n",
        "# compile model with optimizer, loss values for each task, loss \n",
        "# weights for each task.\n",
        "model2.compile(\n",
        "    optimizer=Adam(lr=0.01),\n",
        "    loss={'type': 'sparse_categorical_crossentropy',\n",
        "          'price': 'sparse_categorical_crossentropy'\n",
        "          },\n",
        "    loss_weights={\n",
        "        'type': 0.7,\n",
        "        'price': 0.3},\n",
        "    metrics={\n",
        "        'type': ['SparseCategoricalAccuracy'],\n",
        "        'price': ['SparseCategoricalAccuracy'],\n",
        "        },\n",
        "    )\n",
        "\n",
        "\n",
        "model2.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRPc-Jn_1As1",
        "outputId": "eacc1057-b539-462a-e579-09b46ecbdf1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 64, 64, 2)]  0           []                               \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 57, 57, 32)   4128        ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " max_pooling2d_1 (MaxPooling2D)  (None, 7, 7, 32)    0           ['conv2d_2[0][0]']               \n",
            "                                                                                                  \n",
            " input_1 (InputLayer)           [(None, 100)]        0           []                               \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 7, 7, 32)     0           ['max_pooling2d_1[0][0]']        \n",
            "                                                                                                  \n",
            " embedding_4 (Embedding)        (None, 100, 250)     10000000    ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 4, 4, 32)     16416       ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            " spatial_dropout1d (SpatialDrop  (None, 100, 250)    0           ['embedding_4[0][0]']            \n",
            " out1D)                                                                                           \n",
            "                                                                                                  \n",
            " max_pooling2d_2 (MaxPooling2D)  (None, 1, 1, 32)    0           ['conv2d_3[0][0]']               \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    (None, 100, 32)      36224       ['spatial_dropout1d[0][0]']      \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 1, 1, 32)     0           ['max_pooling2d_2[0][0]']        \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_2 (TFOpLam  (None, 32)          0           ['lstm[0][0]']                   \n",
            " bda)                                                                                             \n",
            "                                                                                                  \n",
            " flatten_2 (Flatten)            (None, 32)           0           ['dropout_1[0][0]']              \n",
            "                                                                                                  \n",
            " tf.concat_2 (TFOpLambda)       (None, 64)           0           ['tf.math.reduce_mean_2[0][0]',  \n",
            "                                                                  'flatten_2[0][0]']              \n",
            "                                                                                                  \n",
            " price (Dense)                  (None, 3)            195         ['tf.concat_2[0][0]']            \n",
            "                                                                                                  \n",
            " type (Dense)                   (None, 24)           1560        ['tf.concat_2[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 10,058,523\n",
            "Trainable params: 10,058,523\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training\n",
        "\n",
        "Based on the training/validation performance, you can adjust the epochs to be trained. Early stoping is watching the validation loss on genre prediction (assuming that it is the main task we would like to perform)"
      ],
      "metadata": {
        "id": "vmXhVyJU37Bj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model2.fit(\n",
        "    x={\n",
        "        'summary': x_train_text_id,\n",
        "        'image': x_train_image\n",
        "    },\n",
        "    y={\n",
        "        'type': y_train_type,\n",
        "        'price': y_train_price,\n",
        "    },\n",
        "    epochs=20,\n",
        "    batch_size=16,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_genre_loss', patience=5, )\n",
        "    ],\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOJzsJ-t4wAN",
        "outputId": "e0d23f34-5e8e-44b9-92bd-134ace717e6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 7.0447 - price_loss: 5.8377 - type_loss: 6.5044 - price_sparse_categorical_accuracy: 0.5825 - type_sparse_categorical_accuracy: 0.6987"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 53s 125ms/step - loss: 7.0447 - price_loss: 5.8377 - type_loss: 6.5044 - price_sparse_categorical_accuracy: 0.5825 - type_sparse_categorical_accuracy: 0.6987 - val_loss: 1.5915 - val_price_loss: 0.8117 - val_type_loss: 0.9372 - val_price_sparse_categorical_accuracy: 0.6252 - val_type_sparse_categorical_accuracy: 0.7647\n",
            "Epoch 2/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 1.5633 - price_loss: 0.8070 - type_loss: 0.9277 - price_sparse_categorical_accuracy: 0.6291 - type_sparse_categorical_accuracy: 0.7528"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 15s 40ms/step - loss: 1.5633 - price_loss: 0.8070 - type_loss: 0.9277 - price_sparse_categorical_accuracy: 0.6291 - type_sparse_categorical_accuracy: 0.7528 - val_loss: 1.4966 - val_price_loss: 0.7783 - val_type_loss: 0.8712 - val_price_sparse_categorical_accuracy: 0.6363 - val_type_sparse_categorical_accuracy: 0.7667\n",
            "Epoch 3/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 1.4435 - price_loss: 0.7759 - type_loss: 0.8205 - price_sparse_categorical_accuracy: 0.6532 - type_sparse_categorical_accuracy: 0.7651"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 10s 27ms/step - loss: 1.4435 - price_loss: 0.7759 - type_loss: 0.8205 - price_sparse_categorical_accuracy: 0.6532 - type_sparse_categorical_accuracy: 0.7651 - val_loss: 1.4323 - val_price_loss: 0.7735 - val_type_loss: 0.8296 - val_price_sparse_categorical_accuracy: 0.6448 - val_type_sparse_categorical_accuracy: 0.7693\n",
            "Epoch 4/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 1.3435 - price_loss: 0.7527 - type_loss: 0.7357 - price_sparse_categorical_accuracy: 0.6573 - type_sparse_categorical_accuracy: 0.7859"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 10s 25ms/step - loss: 1.3435 - price_loss: 0.7527 - type_loss: 0.7357 - price_sparse_categorical_accuracy: 0.6573 - type_sparse_categorical_accuracy: 0.7859 - val_loss: 1.3870 - val_price_loss: 0.7497 - val_type_loss: 0.8235 - val_price_sparse_categorical_accuracy: 0.6533 - val_type_sparse_categorical_accuracy: 0.7713\n",
            "Epoch 5/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 1.2151 - price_loss: 0.7025 - type_loss: 0.6225 - price_sparse_categorical_accuracy: 0.6941 - type_sparse_categorical_accuracy: 0.8263"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 8s 22ms/step - loss: 1.2151 - price_loss: 0.7025 - type_loss: 0.6225 - price_sparse_categorical_accuracy: 0.6941 - type_sparse_categorical_accuracy: 0.8263 - val_loss: 1.3467 - val_price_loss: 0.7432 - val_type_loss: 0.8179 - val_price_sparse_categorical_accuracy: 0.6743 - val_type_sparse_categorical_accuracy: 0.7792\n",
            "Epoch 6/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 1.0965 - price_loss: 0.6583 - type_loss: 0.5218 - price_sparse_categorical_accuracy: 0.7233 - type_sparse_categorical_accuracy: 0.8577"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 5s 14ms/step - loss: 1.0965 - price_loss: 0.6583 - type_loss: 0.5218 - price_sparse_categorical_accuracy: 0.7233 - type_sparse_categorical_accuracy: 0.8577 - val_loss: 1.3532 - val_price_loss: 0.7370 - val_type_loss: 0.8800 - val_price_sparse_categorical_accuracy: 0.6743 - val_type_sparse_categorical_accuracy: 0.7700\n",
            "Epoch 7/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 1.0047 - price_loss: 0.6170 - type_loss: 0.4582 - price_sparse_categorical_accuracy: 0.7463 - type_sparse_categorical_accuracy: 0.8779"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 7s 19ms/step - loss: 1.0047 - price_loss: 0.6170 - type_loss: 0.4582 - price_sparse_categorical_accuracy: 0.7463 - type_sparse_categorical_accuracy: 0.8779 - val_loss: 1.3294 - val_price_loss: 0.7488 - val_type_loss: 0.8909 - val_price_sparse_categorical_accuracy: 0.6638 - val_type_sparse_categorical_accuracy: 0.7621\n",
            "Epoch 8/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 0.9121 - price_loss: 0.5648 - type_loss: 0.3996 - price_sparse_categorical_accuracy: 0.7705 - type_sparse_categorical_accuracy: 0.8921"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 6s 15ms/step - loss: 0.9121 - price_loss: 0.5648 - type_loss: 0.3996 - price_sparse_categorical_accuracy: 0.7705 - type_sparse_categorical_accuracy: 0.8921 - val_loss: 1.3388 - val_price_loss: 0.7654 - val_type_loss: 0.9497 - val_price_sparse_categorical_accuracy: 0.6684 - val_type_sparse_categorical_accuracy: 0.7641\n",
            "Epoch 9/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 0.8262 - price_loss: 0.5181 - type_loss: 0.3495 - price_sparse_categorical_accuracy: 0.8005 - type_sparse_categorical_accuracy: 0.9080"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 7s 19ms/step - loss: 0.8262 - price_loss: 0.5181 - type_loss: 0.3495 - price_sparse_categorical_accuracy: 0.8005 - type_sparse_categorical_accuracy: 0.9080 - val_loss: 1.3655 - val_price_loss: 0.8025 - val_type_loss: 1.0246 - val_price_sparse_categorical_accuracy: 0.6547 - val_type_sparse_categorical_accuracy: 0.7484\n",
            "Epoch 10/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 0.7451 - price_loss: 0.4717 - type_loss: 0.3070 - price_sparse_categorical_accuracy: 0.8208 - type_sparse_categorical_accuracy: 0.9171"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 5s 14ms/step - loss: 0.7451 - price_loss: 0.4717 - type_loss: 0.3070 - price_sparse_categorical_accuracy: 0.8208 - type_sparse_categorical_accuracy: 0.9171 - val_loss: 1.3901 - val_price_loss: 0.8892 - val_type_loss: 1.0773 - val_price_sparse_categorical_accuracy: 0.6356 - val_type_sparse_categorical_accuracy: 0.7484\n",
            "Epoch 11/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 0.6714 - price_loss: 0.4327 - type_loss: 0.2734 - price_sparse_categorical_accuracy: 0.8374 - type_sparse_categorical_accuracy: 0.9271"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 7s 18ms/step - loss: 0.6714 - price_loss: 0.4327 - type_loss: 0.2734 - price_sparse_categorical_accuracy: 0.8374 - type_sparse_categorical_accuracy: 0.9271 - val_loss: 1.3828 - val_price_loss: 0.8757 - val_type_loss: 1.1276 - val_price_sparse_categorical_accuracy: 0.6494 - val_type_sparse_categorical_accuracy: 0.7562\n",
            "Epoch 12/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 0.6011 - price_loss: 0.3995 - type_loss: 0.2434 - price_sparse_categorical_accuracy: 0.8485 - type_sparse_categorical_accuracy: 0.9395"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 6s 15ms/step - loss: 0.6011 - price_loss: 0.3995 - type_loss: 0.2434 - price_sparse_categorical_accuracy: 0.8485 - type_sparse_categorical_accuracy: 0.9395 - val_loss: 1.3428 - val_price_loss: 0.9381 - val_type_loss: 1.1010 - val_price_sparse_categorical_accuracy: 0.6566 - val_type_sparse_categorical_accuracy: 0.7569\n",
            "Epoch 13/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 0.5351 - price_loss: 0.3708 - type_loss: 0.2185 - price_sparse_categorical_accuracy: 0.8602 - type_sparse_categorical_accuracy: 0.9441"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 5s 14ms/step - loss: 0.5351 - price_loss: 0.3708 - type_loss: 0.2185 - price_sparse_categorical_accuracy: 0.8602 - type_sparse_categorical_accuracy: 0.9441 - val_loss: 1.4025 - val_price_loss: 1.0534 - val_type_loss: 1.1937 - val_price_sparse_categorical_accuracy: 0.6225 - val_type_sparse_categorical_accuracy: 0.7477\n",
            "Epoch 14/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 0.4669 - price_loss: 0.3438 - type_loss: 0.1892 - price_sparse_categorical_accuracy: 0.8685 - type_sparse_categorical_accuracy: 0.9518"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 7s 20ms/step - loss: 0.4669 - price_loss: 0.3438 - type_loss: 0.1892 - price_sparse_categorical_accuracy: 0.8685 - type_sparse_categorical_accuracy: 0.9518 - val_loss: 1.3700 - val_price_loss: 1.0365 - val_type_loss: 1.2103 - val_price_sparse_categorical_accuracy: 0.6494 - val_type_sparse_categorical_accuracy: 0.7516\n",
            "Epoch 15/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 0.4122 - price_loss: 0.3215 - type_loss: 0.1750 - price_sparse_categorical_accuracy: 0.8753 - type_sparse_categorical_accuracy: 0.9562"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 6s 17ms/step - loss: 0.4122 - price_loss: 0.3215 - type_loss: 0.1750 - price_sparse_categorical_accuracy: 0.8753 - type_sparse_categorical_accuracy: 0.9562 - val_loss: 1.3557 - val_price_loss: 1.0587 - val_type_loss: 1.2323 - val_price_sparse_categorical_accuracy: 0.6429 - val_type_sparse_categorical_accuracy: 0.7693\n",
            "Epoch 16/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 0.3574 - price_loss: 0.2899 - type_loss: 0.1606 - price_sparse_categorical_accuracy: 0.8856 - type_sparse_categorical_accuracy: 0.9590"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 7s 18ms/step - loss: 0.3574 - price_loss: 0.2899 - type_loss: 0.1606 - price_sparse_categorical_accuracy: 0.8856 - type_sparse_categorical_accuracy: 0.9590 - val_loss: 1.3884 - val_price_loss: 1.1715 - val_type_loss: 1.2800 - val_price_sparse_categorical_accuracy: 0.6350 - val_type_sparse_categorical_accuracy: 0.7615\n",
            "Epoch 17/20\n",
            "381/382 [============================>.] - ETA: 0s - loss: 0.3211 - price_loss: 0.2749 - type_loss: 0.1549 - price_sparse_categorical_accuracy: 0.8963 - type_sparse_categorical_accuracy: 0.9618"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 5s 13ms/step - loss: 0.3210 - price_loss: 0.2747 - type_loss: 0.1550 - price_sparse_categorical_accuracy: 0.8964 - type_sparse_categorical_accuracy: 0.9618 - val_loss: 1.4474 - val_price_loss: 1.2346 - val_type_loss: 1.3645 - val_price_sparse_categorical_accuracy: 0.6298 - val_type_sparse_categorical_accuracy: 0.7471\n",
            "Epoch 18/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 0.2781 - price_loss: 0.2404 - type_loss: 0.1371 - price_sparse_categorical_accuracy: 0.9092 - type_sparse_categorical_accuracy: 0.9644"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 6s 14ms/step - loss: 0.2781 - price_loss: 0.2404 - type_loss: 0.1371 - price_sparse_categorical_accuracy: 0.9092 - type_sparse_categorical_accuracy: 0.9644 - val_loss: 1.4210 - val_price_loss: 1.2711 - val_type_loss: 1.3446 - val_price_sparse_categorical_accuracy: 0.6415 - val_type_sparse_categorical_accuracy: 0.7595\n",
            "Epoch 19/20\n",
            "380/382 [============================>.] - ETA: 0s - loss: 0.2462 - price_loss: 0.2211 - type_loss: 0.1309 - price_sparse_categorical_accuracy: 0.9183 - type_sparse_categorical_accuracy: 0.9661"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 6s 15ms/step - loss: 0.2463 - price_loss: 0.2209 - type_loss: 0.1311 - price_sparse_categorical_accuracy: 0.9184 - type_sparse_categorical_accuracy: 0.9661 - val_loss: 1.4103 - val_price_loss: 1.3624 - val_type_loss: 1.3191 - val_price_sparse_categorical_accuracy: 0.6396 - val_type_sparse_categorical_accuracy: 0.7641\n",
            "Epoch 20/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 0.2132 - price_loss: 0.1933 - type_loss: 0.1225 - price_sparse_categorical_accuracy: 0.9254 - type_sparse_categorical_accuracy: 0.9687"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 5s 14ms/step - loss: 0.2132 - price_loss: 0.1933 - type_loss: 0.1225 - price_sparse_categorical_accuracy: 0.9254 - type_sparse_categorical_accuracy: 0.9687 - val_loss: 1.4874 - val_price_loss: 1.4429 - val_type_loss: 1.4193 - val_price_sparse_categorical_accuracy: 0.6245 - val_type_sparse_categorical_accuracy: 0.7510\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predition\n",
        "\n",
        "We can use the model to predict the testing samples."
      ],
      "metadata": {
        "id": "CxZjmhBv5ElL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McneoJ1Q5ElM",
        "outputId": "adc2257a-c767-4d9b-9601-05248811a3da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "230/230 [==============================] - 2s 4ms/step\n",
            "[[9.89170372e-01 8.42199102e-03 2.40763207e-03]\n",
            " [9.99873042e-01 2.47895059e-05 1.02198166e-04]\n",
            " [9.98946846e-01 6.18430611e-04 4.34806250e-04]\n",
            " ...\n",
            " [7.24591613e-01 2.45650113e-01 2.97582727e-02]\n",
            " [9.99966621e-01 2.18023506e-05 1.15584207e-05]\n",
            " [2.64426976e-01 4.71234798e-01 2.64338225e-01]]\n",
            "[0 0 0 ... 0 0 1]\n"
          ]
        }
      ],
      "source": [
        "# we can do prediction on training set\n",
        "y_predict2 = model2.predict(\n",
        "    {\n",
        "        'summary': x_test_text,\n",
        "        'image': x_test_image\n",
        "    }\n",
        ")\n",
        "\n",
        "\n",
        "# probabilities\n",
        "price_predicted2 = y_predict2['price']\n",
        "print(price_predicted2)\n",
        "\n",
        "# categories\n",
        "price_category_predicted2 = np.argmax(price_predicted2, axis=1)\n",
        "print(price_category_predicted2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJaP8QH15ElM"
      },
      "outputs": [],
      "source": [
        "#  (if for kaggle competition and it is about genre prediction)\n",
        "pd.DataFrame(\n",
        "    {'id': test.id,\n",
        "     'price': price_category_predicted2}\n",
        ").to_csv('sample_submission2.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This Trail Get  Score on Kaggle (0.63831)\n",
        "\n",
        "The score without Lemmatization (0.61603)"
      ],
      "metadata": {
        "id": "W9-93JQfQofB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It get score better than the pervious model and the lemmatization method help it to get the better score."
      ],
      "metadata": {
        "id": "y0b0uIRwVI2r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#The Third Trial"
      ],
      "metadata": {
        "id": "V7m8Knt95ElM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multi-modality learning with Multi-objective learning**\n",
        "\n",
        "\n",
        "*   Multi-modality learning with multi-objective learning can be more versatile and comprehensive, as it can combine the benefits of both approaches and handle complex problems that involve multiple input data sources and multiple output tasks. However, it may also face some challenges, such as designing a suitable model architecture that can fuse and optimize multiple modalities and tasks, tuning the hyperparameters and loss functions for each modality and task, and evaluating the performance and trade-offs of the model.\n",
        "*   It's take summary and image as an input and predict price and type as output.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gazh_KDNBNmD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using BiDirectional layer and GRU layer"
      ],
      "metadata": {
        "id": "dR4pTuYZJsOK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bidirectional recurrent neural networks (BRNN) connect two hidden layers of opposite directions to the same output. \n",
        "BRNN are especially useful when the context of the input is needed. For example, in handwriting recognition, the performance can be enhanced by knowledge of the letters located before and after the current letter."
      ],
      "metadata": {
        "id": "x1OydYdEJutJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A GRU layer is an RNN layer that learns dependencies between time steps in time series and sequence data."
      ],
      "metadata": {
        "id": "mWSNvAYSOdpJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GRU is less complex than LSTM because it has less number of gates. (This may result in better accuracy)"
      ],
      "metadata": {
        "id": "to05uFIxOiWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import GRU, Bidirectional\n",
        "\n",
        "from keras.layers import SimpleRNN\n",
        "\n",
        "# text part\n",
        "# simple average of embedding\n",
        "embedded = keras.layers.Embedding(tokenizer.num_words, 250)(in_text)\n",
        "spatial=SpatialDropout1D(0.2)(embedded)\n",
        "BRNN = Bidirectional(GRU(128,dropout=0.2, recurrent_regularizer='l2'))(spatial)\n",
        "\n",
        "\n",
        "# multi-task learning (each is a multi-class classification)\n",
        "# one dense layer for each task\n",
        "p_type = Dense(len_type, activation='softmax', name='type')(BRNN)\n",
        "p_price = Dense(len_price, activation='softmax', name='price')(BRNN)\n",
        "\n",
        "\n",
        "# define model input/output using keys.\n",
        "model3= keras.Model(\n",
        "    inputs={\n",
        "        'summary': in_text,\n",
        "        'image': in_image\n",
        "        },\n",
        "    outputs={'type': p_type,\n",
        "             'price': p_price,\n",
        "             },\n",
        "    )\n",
        "\n",
        "# compile model with optimizer, loss values for each task, loss \n",
        "# weights for each task.\n",
        "model3.compile(\n",
        "    optimizer=Adam(lr=0.001),\n",
        "    loss={'type': 'sparse_categorical_crossentropy',\n",
        "          'price': 'sparse_categorical_crossentropy'\n",
        "          },\n",
        "    loss_weights={'type': 0.5,\n",
        "                  'price': 0.5\n",
        "                  },\n",
        "    metrics={'type': ['SparseCategoricalAccuracy'],\n",
        "             'price': ['SparseCategoricalAccuracy'],\n",
        "             },\n",
        "    )\n",
        "\n",
        "model3.summary()\n",
        "     "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IoX-me03-fFb",
        "outputId": "d1675c5a-6e24-4565-ff5d-f048c42354b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 100)]        0           []                               \n",
            "                                                                                                  \n",
            " embedding_6 (Embedding)        (None, 100, 250)     10000000    ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " spatial_dropout1d_1 (SpatialDr  (None, 100, 250)    0           ['embedding_6[0][0]']            \n",
            " opout1D)                                                                                         \n",
            "                                                                                                  \n",
            " bidirectional (Bidirectional)  (None, 256)          291840      ['spatial_dropout1d_1[0][0]']    \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 64, 64, 2)]  0           []                               \n",
            "                                                                                                  \n",
            " price (Dense)                  (None, 3)            771         ['bidirectional[0][0]']          \n",
            "                                                                                                  \n",
            " type (Dense)                   (None, 24)           6168        ['bidirectional[0][0]']          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 10,298,779\n",
            "Trainable params: 10,298,779\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training\n",
        "\n",
        "Based on the training/validation performance, you can adjust the epochs to be trained. Early stoping is watching the validation loss on genre prediction (assuming that it is the main task we would like to perform)"
      ],
      "metadata": {
        "id": "aFKVoYbjPfLY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model3.fit(\n",
        "    x={\n",
        "        'summary': x_train_text_id,\n",
        "        'image': x_train_image\n",
        "    },\n",
        "    y={\n",
        "        'type': y_train_type,\n",
        "        'price': y_train_price,\n",
        "    },\n",
        "    epochs=25,\n",
        "    batch_size=16,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_genre_loss', patience=5, )\n",
        "    ],\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bbf0e1e-c658-4063-f4c1-e87322f48a8c",
        "id": "f6U35i2hPfLY"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "382/382 [==============================] - ETA: 0s - loss: 1.1733 - price_loss: 0.8155 - type_loss: 1.0496 - price_sparse_categorical_accuracy: 0.6281 - type_sparse_categorical_accuracy: 0.7517"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 43s 98ms/step - loss: 1.1733 - price_loss: 0.8155 - type_loss: 1.0496 - price_sparse_categorical_accuracy: 0.6281 - type_sparse_categorical_accuracy: 0.7517 - val_loss: 0.8347 - val_price_loss: 0.7652 - val_type_loss: 0.8938 - val_price_sparse_categorical_accuracy: 0.6448 - val_type_sparse_categorical_accuracy: 0.7700\n",
            "Epoch 2/25\n",
            "382/382 [==============================] - ETA: 0s - loss: 0.7948 - price_loss: 0.7301 - type_loss: 0.8359 - price_sparse_categorical_accuracy: 0.6684 - type_sparse_categorical_accuracy: 0.7735"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 10s 26ms/step - loss: 0.7948 - price_loss: 0.7301 - type_loss: 0.8359 - price_sparse_categorical_accuracy: 0.6684 - type_sparse_categorical_accuracy: 0.7735 - val_loss: 0.7982 - val_price_loss: 0.7307 - val_type_loss: 0.8489 - val_price_sparse_categorical_accuracy: 0.6710 - val_type_sparse_categorical_accuracy: 0.7785\n",
            "Epoch 3/25\n",
            "382/382 [==============================] - ETA: 0s - loss: 0.6964 - price_loss: 0.6586 - type_loss: 0.7112 - price_sparse_categorical_accuracy: 0.7161 - type_sparse_categorical_accuracy: 0.7954"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 9s 22ms/step - loss: 0.6964 - price_loss: 0.6586 - type_loss: 0.7112 - price_sparse_categorical_accuracy: 0.7161 - type_sparse_categorical_accuracy: 0.7954 - val_loss: 0.8054 - val_price_loss: 0.7580 - val_type_loss: 0.8355 - val_price_sparse_categorical_accuracy: 0.6311 - val_type_sparse_categorical_accuracy: 0.7851\n",
            "Epoch 4/25\n",
            "382/382 [==============================] - ETA: 0s - loss: 0.6010 - price_loss: 0.5832 - type_loss: 0.5983 - price_sparse_categorical_accuracy: 0.7527 - type_sparse_categorical_accuracy: 0.8287"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 9s 24ms/step - loss: 0.6010 - price_loss: 0.5832 - type_loss: 0.5983 - price_sparse_categorical_accuracy: 0.7527 - type_sparse_categorical_accuracy: 0.8287 - val_loss: 0.8364 - val_price_loss: 0.7857 - val_type_loss: 0.8734 - val_price_sparse_categorical_accuracy: 0.6566 - val_type_sparse_categorical_accuracy: 0.7674\n",
            "Epoch 5/25\n",
            "382/382 [==============================] - ETA: 0s - loss: 0.5102 - price_loss: 0.5050 - type_loss: 0.4988 - price_sparse_categorical_accuracy: 0.7931 - type_sparse_categorical_accuracy: 0.8590"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 8s 21ms/step - loss: 0.5102 - price_loss: 0.5050 - type_loss: 0.4988 - price_sparse_categorical_accuracy: 0.7931 - type_sparse_categorical_accuracy: 0.8590 - val_loss: 0.9159 - val_price_loss: 0.8807 - val_type_loss: 0.9395 - val_price_sparse_categorical_accuracy: 0.6278 - val_type_sparse_categorical_accuracy: 0.7726\n",
            "Epoch 6/25\n",
            "382/382 [==============================] - ETA: 0s - loss: 0.4174 - price_loss: 0.4225 - type_loss: 0.3976 - price_sparse_categorical_accuracy: 0.8297 - type_sparse_categorical_accuracy: 0.8867"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 8s 20ms/step - loss: 0.4174 - price_loss: 0.4225 - type_loss: 0.3976 - price_sparse_categorical_accuracy: 0.8297 - type_sparse_categorical_accuracy: 0.8867 - val_loss: 1.0539 - val_price_loss: 1.0172 - val_type_loss: 1.0759 - val_price_sparse_categorical_accuracy: 0.6402 - val_type_sparse_categorical_accuracy: 0.7634\n",
            "Epoch 7/25\n",
            "382/382 [==============================] - ETA: 0s - loss: 0.3478 - price_loss: 0.3612 - type_loss: 0.3214 - price_sparse_categorical_accuracy: 0.8543 - type_sparse_categorical_accuracy: 0.9105"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 7s 17ms/step - loss: 0.3478 - price_loss: 0.3612 - type_loss: 0.3214 - price_sparse_categorical_accuracy: 0.8543 - type_sparse_categorical_accuracy: 0.9105 - val_loss: 1.1575 - val_price_loss: 1.1243 - val_type_loss: 1.1791 - val_price_sparse_categorical_accuracy: 0.6271 - val_type_sparse_categorical_accuracy: 0.7398\n",
            "Epoch 8/25\n",
            "381/382 [============================>.] - ETA: 0s - loss: 0.2791 - price_loss: 0.2971 - type_loss: 0.2494 - price_sparse_categorical_accuracy: 0.8804 - type_sparse_categorical_accuracy: 0.9298"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 8s 20ms/step - loss: 0.2795 - price_loss: 0.2972 - type_loss: 0.2501 - price_sparse_categorical_accuracy: 0.8803 - type_sparse_categorical_accuracy: 0.9294 - val_loss: 1.2898 - val_price_loss: 1.2646 - val_type_loss: 1.3053 - val_price_sparse_categorical_accuracy: 0.6173 - val_type_sparse_categorical_accuracy: 0.7425\n",
            "Epoch 9/25\n",
            "382/382 [==============================] - ETA: 0s - loss: 0.2400 - price_loss: 0.2566 - type_loss: 0.2124 - price_sparse_categorical_accuracy: 0.8948 - type_sparse_categorical_accuracy: 0.9415"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 8s 22ms/step - loss: 0.2400 - price_loss: 0.2566 - type_loss: 0.2124 - price_sparse_categorical_accuracy: 0.8948 - type_sparse_categorical_accuracy: 0.9415 - val_loss: 1.4080 - val_price_loss: 1.4186 - val_type_loss: 1.3870 - val_price_sparse_categorical_accuracy: 0.6166 - val_type_sparse_categorical_accuracy: 0.7503\n",
            "Epoch 10/25\n",
            "382/382 [==============================] - ETA: 0s - loss: 0.2034 - price_loss: 0.2249 - type_loss: 0.1738 - price_sparse_categorical_accuracy: 0.9123 - type_sparse_categorical_accuracy: 0.9535"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 11s 28ms/step - loss: 0.2034 - price_loss: 0.2249 - type_loss: 0.1738 - price_sparse_categorical_accuracy: 0.9123 - type_sparse_categorical_accuracy: 0.9535 - val_loss: 1.5475 - val_price_loss: 1.5608 - val_type_loss: 1.5267 - val_price_sparse_categorical_accuracy: 0.5911 - val_type_sparse_categorical_accuracy: 0.7248\n",
            "Epoch 11/25\n",
            "382/382 [==============================] - ETA: 0s - loss: 0.1798 - price_loss: 0.1965 - type_loss: 0.1552 - price_sparse_categorical_accuracy: 0.9266 - type_sparse_categorical_accuracy: 0.9575"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 9s 23ms/step - loss: 0.1798 - price_loss: 0.1965 - type_loss: 0.1552 - price_sparse_categorical_accuracy: 0.9266 - type_sparse_categorical_accuracy: 0.9575 - val_loss: 1.6626 - val_price_loss: 1.7100 - val_type_loss: 1.6077 - val_price_sparse_categorical_accuracy: 0.6166 - val_type_sparse_categorical_accuracy: 0.7353\n",
            "Epoch 12/25\n",
            "382/382 [==============================] - ETA: 0s - loss: 0.1612 - price_loss: 0.1818 - type_loss: 0.1339 - price_sparse_categorical_accuracy: 0.9279 - type_sparse_categorical_accuracy: 0.9651"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 8s 22ms/step - loss: 0.1612 - price_loss: 0.1818 - type_loss: 0.1339 - price_sparse_categorical_accuracy: 0.9279 - type_sparse_categorical_accuracy: 0.9651 - val_loss: 1.8087 - val_price_loss: 1.8688 - val_type_loss: 1.7422 - val_price_sparse_categorical_accuracy: 0.5957 - val_type_sparse_categorical_accuracy: 0.7425\n",
            "Epoch 13/25\n",
            "382/382 [==============================] - ETA: 0s - loss: 0.1478 - price_loss: 0.1644 - type_loss: 0.1246 - price_sparse_categorical_accuracy: 0.9351 - type_sparse_categorical_accuracy: 0.9675"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 9s 24ms/step - loss: 0.1478 - price_loss: 0.1644 - type_loss: 0.1246 - price_sparse_categorical_accuracy: 0.9351 - type_sparse_categorical_accuracy: 0.9675 - val_loss: 1.8499 - val_price_loss: 1.9313 - val_type_loss: 1.7602 - val_price_sparse_categorical_accuracy: 0.5740 - val_type_sparse_categorical_accuracy: 0.7379\n",
            "Epoch 14/25\n",
            "382/382 [==============================] - ETA: 0s - loss: 0.1398 - price_loss: 0.1568 - type_loss: 0.1166 - price_sparse_categorical_accuracy: 0.9366 - type_sparse_categorical_accuracy: 0.9712"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 10s 25ms/step - loss: 0.1398 - price_loss: 0.1568 - type_loss: 0.1166 - price_sparse_categorical_accuracy: 0.9366 - type_sparse_categorical_accuracy: 0.9712 - val_loss: 1.9805 - val_price_loss: 2.0839 - val_type_loss: 1.8671 - val_price_sparse_categorical_accuracy: 0.6003 - val_type_sparse_categorical_accuracy: 0.7280\n",
            "Epoch 15/25\n",
            "382/382 [==============================] - ETA: 0s - loss: 0.1319 - price_loss: 0.1467 - type_loss: 0.1108 - price_sparse_categorical_accuracy: 0.9477 - type_sparse_categorical_accuracy: 0.9708"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 9s 23ms/step - loss: 0.1319 - price_loss: 0.1467 - type_loss: 0.1108 - price_sparse_categorical_accuracy: 0.9477 - type_sparse_categorical_accuracy: 0.9708 - val_loss: 2.0135 - val_price_loss: 2.1313 - val_type_loss: 1.8878 - val_price_sparse_categorical_accuracy: 0.5832 - val_type_sparse_categorical_accuracy: 0.7300\n",
            "Epoch 16/25\n",
            "382/382 [==============================] - ETA: 0s - loss: 0.1248 - price_loss: 0.1384 - type_loss: 0.1054 - price_sparse_categorical_accuracy: 0.9479 - type_sparse_categorical_accuracy: 0.9716"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 8s 21ms/step - loss: 0.1248 - price_loss: 0.1384 - type_loss: 0.1054 - price_sparse_categorical_accuracy: 0.9479 - type_sparse_categorical_accuracy: 0.9716 - val_loss: 2.1180 - val_price_loss: 2.3082 - val_type_loss: 1.9190 - val_price_sparse_categorical_accuracy: 0.5937 - val_type_sparse_categorical_accuracy: 0.7267\n",
            "Epoch 17/25\n",
            "382/382 [==============================] - ETA: 0s - loss: 0.1213 - price_loss: 0.1353 - type_loss: 0.1013 - price_sparse_categorical_accuracy: 0.9475 - type_sparse_categorical_accuracy: 0.9743"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 7s 19ms/step - loss: 0.1213 - price_loss: 0.1353 - type_loss: 0.1013 - price_sparse_categorical_accuracy: 0.9475 - type_sparse_categorical_accuracy: 0.9743 - val_loss: 2.1412 - val_price_loss: 2.3309 - val_type_loss: 1.9405 - val_price_sparse_categorical_accuracy: 0.5865 - val_type_sparse_categorical_accuracy: 0.7254\n",
            "Epoch 18/25\n",
            "382/382 [==============================] - ETA: 0s - loss: 0.1153 - price_loss: 0.1277 - type_loss: 0.0973 - price_sparse_categorical_accuracy: 0.9487 - type_sparse_categorical_accuracy: 0.9759"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 9s 25ms/step - loss: 0.1153 - price_loss: 0.1277 - type_loss: 0.0973 - price_sparse_categorical_accuracy: 0.9487 - type_sparse_categorical_accuracy: 0.9759 - val_loss: 2.1938 - val_price_loss: 2.3964 - val_type_loss: 1.9841 - val_price_sparse_categorical_accuracy: 0.5852 - val_type_sparse_categorical_accuracy: 0.7221\n",
            "Epoch 19/25\n",
            "382/382 [==============================] - ETA: 0s - loss: 0.1124 - price_loss: 0.1278 - type_loss: 0.0911 - price_sparse_categorical_accuracy: 0.9513 - type_sparse_categorical_accuracy: 0.9748"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 8s 22ms/step - loss: 0.1124 - price_loss: 0.1278 - type_loss: 0.0911 - price_sparse_categorical_accuracy: 0.9513 - type_sparse_categorical_accuracy: 0.9748 - val_loss: 2.2154 - val_price_loss: 2.4090 - val_type_loss: 2.0172 - val_price_sparse_categorical_accuracy: 0.5780 - val_type_sparse_categorical_accuracy: 0.7261\n",
            "Epoch 20/25\n",
            "382/382 [==============================] - ETA: 0s - loss: 0.1075 - price_loss: 0.1204 - type_loss: 0.0892 - price_sparse_categorical_accuracy: 0.9544 - type_sparse_categorical_accuracy: 0.9774"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 9s 23ms/step - loss: 0.1075 - price_loss: 0.1204 - type_loss: 0.0892 - price_sparse_categorical_accuracy: 0.9544 - type_sparse_categorical_accuracy: 0.9774 - val_loss: 2.2572 - val_price_loss: 2.4738 - val_type_loss: 2.0310 - val_price_sparse_categorical_accuracy: 0.5904 - val_type_sparse_categorical_accuracy: 0.7136\n",
            "Epoch 21/25\n",
            "382/382 [==============================] - ETA: 0s - loss: 0.1084 - price_loss: 0.1199 - type_loss: 0.0909 - price_sparse_categorical_accuracy: 0.9508 - type_sparse_categorical_accuracy: 0.9759"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 7s 19ms/step - loss: 0.1084 - price_loss: 0.1199 - type_loss: 0.0909 - price_sparse_categorical_accuracy: 0.9508 - type_sparse_categorical_accuracy: 0.9759 - val_loss: 2.3120 - val_price_loss: 2.5352 - val_type_loss: 2.0813 - val_price_sparse_categorical_accuracy: 0.5858 - val_type_sparse_categorical_accuracy: 0.7320\n",
            "Epoch 22/25\n",
            "382/382 [==============================] - ETA: 0s - loss: 0.1054 - price_loss: 0.1140 - type_loss: 0.0911 - price_sparse_categorical_accuracy: 0.9543 - type_sparse_categorical_accuracy: 0.9752"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 6s 15ms/step - loss: 0.1054 - price_loss: 0.1140 - type_loss: 0.0911 - price_sparse_categorical_accuracy: 0.9543 - type_sparse_categorical_accuracy: 0.9752 - val_loss: 2.3773 - val_price_loss: 2.6145 - val_type_loss: 2.1321 - val_price_sparse_categorical_accuracy: 0.5465 - val_type_sparse_categorical_accuracy: 0.7307\n",
            "Epoch 23/25\n",
            "382/382 [==============================] - ETA: 0s - loss: 0.1017 - price_loss: 0.1125 - type_loss: 0.0855 - price_sparse_categorical_accuracy: 0.9567 - type_sparse_categorical_accuracy: 0.9779"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 8s 20ms/step - loss: 0.1017 - price_loss: 0.1125 - type_loss: 0.0855 - price_sparse_categorical_accuracy: 0.9567 - type_sparse_categorical_accuracy: 0.9779 - val_loss: 2.4150 - val_price_loss: 2.6544 - val_type_loss: 2.1687 - val_price_sparse_categorical_accuracy: 0.5826 - val_type_sparse_categorical_accuracy: 0.7208\n",
            "Epoch 24/25\n",
            "382/382 [==============================] - ETA: 0s - loss: 0.0985 - price_loss: 0.1094 - type_loss: 0.0826 - price_sparse_categorical_accuracy: 0.9549 - type_sparse_categorical_accuracy: 0.9780"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 7s 17ms/step - loss: 0.0985 - price_loss: 0.1094 - type_loss: 0.0826 - price_sparse_categorical_accuracy: 0.9549 - type_sparse_categorical_accuracy: 0.9780 - val_loss: 2.4087 - val_price_loss: 2.6517 - val_type_loss: 2.1576 - val_price_sparse_categorical_accuracy: 0.5891 - val_type_sparse_categorical_accuracy: 0.7326\n",
            "Epoch 25/25\n",
            "382/382 [==============================] - ETA: 0s - loss: 0.0983 - price_loss: 0.1058 - type_loss: 0.0849 - price_sparse_categorical_accuracy: 0.9577 - type_sparse_categorical_accuracy: 0.9769"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 7s 19ms/step - loss: 0.0983 - price_loss: 0.1058 - type_loss: 0.0849 - price_sparse_categorical_accuracy: 0.9577 - type_sparse_categorical_accuracy: 0.9769 - val_loss: 2.4243 - val_price_loss: 2.6928 - val_type_loss: 2.1501 - val_price_sparse_categorical_accuracy: 0.5727 - val_type_sparse_categorical_accuracy: 0.7294\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predition\n",
        "\n",
        "We can use the model to predict the testing samples."
      ],
      "metadata": {
        "id": "5gcJ6YLOPfLY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dw2HWKB9PfLY",
        "outputId": "511b4591-048c-40d6-8a39-4cbe2a0aa5dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "230/230 [==============================] - 2s 4ms/step\n",
            "[[2.64004439e-01 7.25804508e-01 1.01910969e-02]\n",
            " [9.99869108e-01 3.07755690e-05 1.00148936e-04]\n",
            " [9.94033515e-01 5.83244441e-03 1.33965048e-04]\n",
            " ...\n",
            " [9.84173641e-02 8.92011762e-01 9.57087055e-03]\n",
            " [1.00000000e+00 2.16623941e-12 2.43145152e-12]\n",
            " [3.74833355e-03 8.15285444e-01 1.80966288e-01]]\n",
            "[1 0 0 ... 1 0 1]\n"
          ]
        }
      ],
      "source": [
        "# we can do prediction on training set\n",
        "y_predict3 = model3.predict(\n",
        "    {\n",
        "        'summary': x_test_text,\n",
        "        'image': x_test_image\n",
        "    }\n",
        ")\n",
        "\n",
        "\n",
        "# probabilities\n",
        "price_predicted3 = y_predict3['price']\n",
        "print(price_predicted3)\n",
        "\n",
        "# categories\n",
        "price_category_predicted3 = np.argmax(price_predicted3, axis=1)\n",
        "print(price_category_predicted3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8UEsqjgAPfLY"
      },
      "outputs": [],
      "source": [
        "#  (if for kaggle competition and it is about genre prediction)\n",
        "pd.DataFrame(\n",
        "    {'id': test.id,\n",
        "     'price': price_category_predicted3}\n",
        ").to_csv('sample_submission3.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This Trail Get  Score on Kaggle (0.59184)\n",
        "\n",
        "there's overfitting, this trail is the worest trail amoung all trail."
      ],
      "metadata": {
        "id": "QRYqfVx7evaG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The fourh trail"
      ],
      "metadata": {
        "id": "7zTwQVJjTDSJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi-modality learning and Single task (price).\n",
        "\n",
        "the model has the same structe as previous models in terms of input and hidden layers but it has only one output since it's single task model\n"
      ],
      "metadata": {
        "id": "tD7bAOadTK6g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the input data is multi-modal and the output is single task, then multi-modality learning and single task can be combined to achieve better results than using only one modality or one task. For example, a model that can fuse text and image data to perform sentiment analysis can be more accurate and robust than a model that only uses text or image data.\n"
      ],
      "metadata": {
        "id": "7bnu_Wig8ox_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Using LSTM"
      ],
      "metadata": {
        "id": "duw3TxDLWUAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# text part\n",
        "# simple average of embedding. you can change it to anything else as needed\n",
        "embedded = keras.layers.Embedding(tokenizer.num_words, 200)(in_text)\n",
        "spatial=SpatialDropout1D(0.2)(embedded)\n",
        "lstm=LSTM(64,dropout=0.2,return_sequences=True)(spatial)\n",
        "averaged = tf.reduce_mean(lstm, axis=1)\n",
        "\n",
        "#images part\n",
        "cov = Conv2D(64, (8, 8),activation='relu',kernel_regularizer=regularizers.l2(0.01),kernel_initializer='he_uniform')(in_image)\n",
        "pl = MaxPool2D(pool_size=(8, 8))(cov)#,strides=(1,1),padding='same')(cov) \n",
        "d1=Dropout(0.25)(pl)\n",
        "cov2 = Conv2D(64, (4, 4),kernel_regularizer=regularizers.l2(0.01),activation='relu')(d1)\n",
        "p2 = MaxPool2D((4, 4))(cov2)#,strides=(1,1),padding='same')(cov2)\n",
        "d2=Dropout(0.25)(p2)\n",
        "flattened = Flatten()(d2)\n",
        "\n",
        "\n",
        "# fusion - combinig both\n",
        "fused = tf.concat([averaged, flattened], axis=-1)\n",
        "\n",
        "# multi-task learning (each is a multi-class classification)\n",
        "# one dense layer for each task\n",
        "p_type = Dense(len_type, activation='softmax', name='type')(fused)\n",
        "p_price = Dense(len_price, activation='softmax', name='price')(fused)\n",
        "\n",
        "\n",
        "# define model input/output using keys.\n",
        "model4 = keras.Model(\n",
        "    inputs={\n",
        "        'summary': in_text,\n",
        "        'image': in_image\n",
        "            },\n",
        "    outputs={\n",
        "        'price': p_price,\n",
        "        },\n",
        "    )\n",
        "\n",
        "# compile model with optimizer, loss values for each task, loss \n",
        "# weights for each task.\n",
        "model4.compile(\n",
        "    optimizer=Adam(lr=0.001),\n",
        "    loss={\n",
        "          'price': 'sparse_categorical_crossentropy'\n",
        "          },\n",
        "    loss_weights={\n",
        "        'price': 0.3},\n",
        "    metrics={\n",
        "       \n",
        "        'price': ['SparseCategoricalAccuracy'],\n",
        "        },\n",
        "    )\n",
        "\n",
        "\n",
        "model4.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79d3d75f-dfa2-4b8d-fc97-0bad234b3221",
        "id": "zHIZAv7rUIlI"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_7\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 64, 64, 2)]  0           []                               \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)              (None, 57, 57, 64)   8256        ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " max_pooling2d_5 (MaxPooling2D)  (None, 7, 7, 64)    0           ['conv2d_7[0][0]']               \n",
            "                                                                                                  \n",
            " input_1 (InputLayer)           [(None, 100)]        0           []                               \n",
            "                                                                                                  \n",
            " dropout_5 (Dropout)            (None, 7, 7, 64)     0           ['max_pooling2d_5[0][0]']        \n",
            "                                                                                                  \n",
            " embedding_9 (Embedding)        (None, 100, 200)     8000000     ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)              (None, 4, 4, 64)     65600       ['dropout_5[0][0]']              \n",
            "                                                                                                  \n",
            " spatial_dropout1d_3 (SpatialDr  (None, 100, 200)    0           ['embedding_9[0][0]']            \n",
            " opout1D)                                                                                         \n",
            "                                                                                                  \n",
            " max_pooling2d_6 (MaxPooling2D)  (None, 1, 1, 64)    0           ['conv2d_8[0][0]']               \n",
            "                                                                                                  \n",
            " lstm_3 (LSTM)                  (None, 100, 64)      67840       ['spatial_dropout1d_3[0][0]']    \n",
            "                                                                                                  \n",
            " dropout_6 (Dropout)            (None, 1, 1, 64)     0           ['max_pooling2d_6[0][0]']        \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_6 (TFOpLam  (None, 64)          0           ['lstm_3[0][0]']                 \n",
            " bda)                                                                                             \n",
            "                                                                                                  \n",
            " flatten_5 (Flatten)            (None, 64)           0           ['dropout_6[0][0]']              \n",
            "                                                                                                  \n",
            " tf.concat_5 (TFOpLambda)       (None, 128)          0           ['tf.math.reduce_mean_6[0][0]',  \n",
            "                                                                  'flatten_5[0][0]']              \n",
            "                                                                                                  \n",
            " price (Dense)                  (None, 3)            387         ['tf.concat_5[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 8,142,083\n",
            "Trainable params: 8,142,083\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training\n",
        "\n",
        "Based on the training/validation performance, you can adjust the epochs to be trained. Early stoping is watching the validation loss on genre prediction (assuming that it is the main task we would like to perform)"
      ],
      "metadata": {
        "id": "gDmHFmt2UIlJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model4.fit(\n",
        "    x={\n",
        "        'summary': x_train_text_id,\n",
        "        'image': x_train_image\n",
        "    },\n",
        "    y={\n",
        "        'price': y_train_price,\n",
        "    },\n",
        "    epochs=15,\n",
        "    batch_size=16,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_genre_loss', patience=5, )\n",
        "    ],\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "694782b7-ccbb-43cd-bcbc-f9ffb49be408",
        "id": "-jZ1eDwOUIlJ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "382/382 [==============================] - ETA: 0s - loss: 5.1825 - sparse_categorical_accuracy: 0.5878"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,sparse_categorical_accuracy,val_loss,val_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 33s 77ms/step - loss: 5.1825 - sparse_categorical_accuracy: 0.5878 - val_loss: 1.5485 - val_sparse_categorical_accuracy: 0.6684\n",
            "Epoch 2/15\n",
            "382/382 [==============================] - ETA: 0s - loss: 1.4577 - sparse_categorical_accuracy: 0.6978"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,sparse_categorical_accuracy,val_loss,val_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 8s 22ms/step - loss: 1.4577 - sparse_categorical_accuracy: 0.6978 - val_loss: 1.3900 - val_sparse_categorical_accuracy: 0.6848\n",
            "Epoch 3/15\n",
            "382/382 [==============================] - ETA: 0s - loss: 1.2966 - sparse_categorical_accuracy: 0.7400"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,sparse_categorical_accuracy,val_loss,val_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 8s 20ms/step - loss: 1.2966 - sparse_categorical_accuracy: 0.7400 - val_loss: 1.2663 - val_sparse_categorical_accuracy: 0.6684\n",
            "Epoch 4/15\n",
            "382/382 [==============================] - ETA: 0s - loss: 1.1542 - sparse_categorical_accuracy: 0.7827"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,sparse_categorical_accuracy,val_loss,val_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 6s 16ms/step - loss: 1.1542 - sparse_categorical_accuracy: 0.7827 - val_loss: 1.1646 - val_sparse_categorical_accuracy: 0.6547\n",
            "Epoch 5/15\n",
            "382/382 [==============================] - ETA: 0s - loss: 1.0218 - sparse_categorical_accuracy: 0.8171"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,sparse_categorical_accuracy,val_loss,val_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 6s 17ms/step - loss: 1.0218 - sparse_categorical_accuracy: 0.8171 - val_loss: 1.0730 - val_sparse_categorical_accuracy: 0.6671\n",
            "Epoch 6/15\n",
            "382/382 [==============================] - ETA: 0s - loss: 0.8982 - sparse_categorical_accuracy: 0.8520"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,sparse_categorical_accuracy,val_loss,val_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 5s 13ms/step - loss: 0.8982 - sparse_categorical_accuracy: 0.8520 - val_loss: 0.9947 - val_sparse_categorical_accuracy: 0.6560\n",
            "Epoch 7/15\n",
            "382/382 [==============================] - ETA: 0s - loss: 0.7864 - sparse_categorical_accuracy: 0.8697"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,sparse_categorical_accuracy,val_loss,val_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 6s 16ms/step - loss: 0.7864 - sparse_categorical_accuracy: 0.8697 - val_loss: 0.9429 - val_sparse_categorical_accuracy: 0.6664\n",
            "Epoch 8/15\n",
            "382/382 [==============================] - ETA: 0s - loss: 0.6777 - sparse_categorical_accuracy: 0.8826"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,sparse_categorical_accuracy,val_loss,val_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 5s 13ms/step - loss: 0.6777 - sparse_categorical_accuracy: 0.8826 - val_loss: 0.8745 - val_sparse_categorical_accuracy: 0.6415\n",
            "Epoch 9/15\n",
            "382/382 [==============================] - ETA: 0s - loss: 0.5770 - sparse_categorical_accuracy: 0.8913"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,sparse_categorical_accuracy,val_loss,val_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 5s 13ms/step - loss: 0.5770 - sparse_categorical_accuracy: 0.8913 - val_loss: 0.8078 - val_sparse_categorical_accuracy: 0.6350\n",
            "Epoch 10/15\n",
            "382/382 [==============================] - ETA: 0s - loss: 0.4822 - sparse_categorical_accuracy: 0.9053"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,sparse_categorical_accuracy,val_loss,val_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 6s 15ms/step - loss: 0.4822 - sparse_categorical_accuracy: 0.9053 - val_loss: 0.6962 - val_sparse_categorical_accuracy: 0.6317\n",
            "Epoch 11/15\n",
            "382/382 [==============================] - ETA: 0s - loss: 0.3967 - sparse_categorical_accuracy: 0.9180"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,sparse_categorical_accuracy,val_loss,val_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 5s 13ms/step - loss: 0.3967 - sparse_categorical_accuracy: 0.9180 - val_loss: 0.7464 - val_sparse_categorical_accuracy: 0.6442\n",
            "Epoch 12/15\n",
            "382/382 [==============================] - ETA: 0s - loss: 0.3198 - sparse_categorical_accuracy: 0.9198"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,sparse_categorical_accuracy,val_loss,val_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 6s 16ms/step - loss: 0.3198 - sparse_categorical_accuracy: 0.9198 - val_loss: 0.6080 - val_sparse_categorical_accuracy: 0.6533\n",
            "Epoch 13/15\n",
            "382/382 [==============================] - ETA: 0s - loss: 0.2552 - sparse_categorical_accuracy: 0.9228"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,sparse_categorical_accuracy,val_loss,val_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 5s 12ms/step - loss: 0.2552 - sparse_categorical_accuracy: 0.9228 - val_loss: 0.6453 - val_sparse_categorical_accuracy: 0.6468\n",
            "Epoch 14/15\n",
            "382/382 [==============================] - ETA: 0s - loss: 0.1981 - sparse_categorical_accuracy: 0.9320"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,sparse_categorical_accuracy,val_loss,val_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 5s 13ms/step - loss: 0.1981 - sparse_categorical_accuracy: 0.9320 - val_loss: 0.6042 - val_sparse_categorical_accuracy: 0.6311\n",
            "Epoch 15/15\n",
            "382/382 [==============================] - ETA: 0s - loss: 0.1502 - sparse_categorical_accuracy: 0.9351"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,sparse_categorical_accuracy,val_loss,val_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 5s 14ms/step - loss: 0.1502 - sparse_categorical_accuracy: 0.9351 - val_loss: 0.5292 - val_sparse_categorical_accuracy: 0.6330\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predition\n",
        "\n",
        "We can use the model to predict the testing samples."
      ],
      "metadata": {
        "id": "wPZKauBVbb3X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "491e4958-8795-42cd-97a5-2d3302e73918",
        "id": "iJNnkVRAUIlJ"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "230/230 [==============================] - 1s 4ms/step\n",
            "[[9.4348401e-01 4.8856866e-02 7.6591331e-03]\n",
            " [9.9489439e-01 2.0758535e-03 3.0298091e-03]\n",
            " [9.9967659e-01 2.2491241e-04 9.8559991e-05]\n",
            " ...\n",
            " [7.3642123e-01 2.4121328e-01 2.2365436e-02]\n",
            " [9.9999917e-01 5.8472614e-07 2.1856457e-07]\n",
            " [9.3931311e-01 4.5507845e-02 1.5179035e-02]]\n",
            "[0 0 0 ... 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "# we can do prediction on training set\n",
        "y_predict4 = model4.predict(\n",
        "    {\n",
        "        'summary': x_test_text,\n",
        "        'image': x_test_image\n",
        "    }\n",
        ")\n",
        "\n",
        "\n",
        "# probabilities\n",
        "price_predicted4 = y_predict4['price']\n",
        "print(price_predicted4)\n",
        "\n",
        "# categories\n",
        "price_category_predicted4 = np.argmax(price_predicted4, axis=1)\n",
        "print(price_category_predicted4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ga1LemD1UIlJ"
      },
      "outputs": [],
      "source": [
        "#  (if for kaggle competition and it is about genre prediction)\n",
        "pd.DataFrame(\n",
        "    {'id': test.id,\n",
        "     'price': price_category_predicted4}\n",
        ").to_csv('sample_submission4.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This Trail Get  Score on Kaggle (0.64157) when (NO.epochs = 20)\n",
        "\n",
        "The score (0.63260) when (NO.epochs= 15)\n"
      ],
      "metadata": {
        "id": "4KPfLqCDUIlJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "it's better than previous models."
      ],
      "metadata": {
        "id": "YwOD7_4gXi0D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Fifth trail"
      ],
      "metadata": {
        "id": "hbzfI5QyWLvB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi-objective learning (multi-task) \n",
        "\n",
        "It's take summary as an input and predict price and type as output."
      ],
      "metadata": {
        "id": "BmeqAAhUWPyh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the input data is single input and the output is multi-objective learning (multi-task), then the model can learn to perform multiple tasks using the same input data. For example, a model that can use text data to perform sentiment analysis, topic classification, and summarization can be more efficient and versatile than a model that only performs one task."
      ],
      "metadata": {
        "id": "vfeLHZay9QWF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using LSTM layer "
      ],
      "metadata": {
        "id": "JH039a54Yfa3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# text part\n",
        "embedded = keras.layers.Embedding(tokenizer.num_words, 100)(in_text) # get our word embeddings\n",
        "BLSTM = Bidirectional(LSTM(units = 128,return_sequences=True))(embedded)\n",
        "averaged = tf.reduce_mean(BLSTM, axis=1)\n",
        "\n",
        "\n",
        "# fusion:\n",
        "fused = Dense(128, activation='relu')(averaged)\n",
        "dropout = Dropout(rate = 0.2)(fused)\n",
        "fused = Dense(256, activation='relu')(dropout)\n",
        "\n",
        "# multi-objectives (each is a multi-class classification)\n",
        "# one dense layer for each task\n",
        "p_price = Dense(len_price, activation='softmax', name='price')(fused) # predict price label 0, 1, or 2\n",
        "p_type = Dense(len_type, activation='softmax', name='type')(fused) # predict rental category label [0-23]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# define model input/output using keys.\n",
        "#the input will be summary (dealing with text)\n",
        "model5 = keras.Model(\n",
        "    inputs={\n",
        "        'summary': in_text,\n",
        "            },\n",
        "    outputs={'type': p_type,\n",
        "             'price': p_price,\n",
        "             },\n",
        "    )\n",
        "\n",
        "# compile model with optimizer, loss values for each task, loss \n",
        "# weights for each task.\n",
        "model5.compile(\n",
        "    optimizer=Adam(lr=0.01),\n",
        "    loss={'type': 'sparse_categorical_crossentropy',\n",
        "          'price': 'sparse_categorical_crossentropy'\n",
        "          },\n",
        "    loss_weights={\n",
        "        'type': 0.5,\n",
        "        'price': 0.5\n",
        "        },\n",
        "    metrics={\n",
        "        'type': ['SparseCategoricalAccuracy'],\n",
        "        'price': ['SparseCategoricalAccuracy'],\n",
        "        },\n",
        "    )\n",
        "\n",
        "model5.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Elfggb3tXD-g",
        "outputId": "f735308d-d53d-41b0-d47c-c5df3d581694"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_6\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 100)]        0           []                               \n",
            "                                                                                                  \n",
            " embedding_8 (Embedding)        (None, 100, 100)     4000000     ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " bidirectional_1 (Bidirectional  (None, 100, 256)    234496      ['embedding_8[0][0]']            \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_5 (TFOpLam  (None, 256)         0           ['bidirectional_1[0][0]']        \n",
            " bda)                                                                                             \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 128)          32896       ['tf.math.reduce_mean_5[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_4 (Dropout)            (None, 128)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 256)          33024       ['dropout_4[0][0]']              \n",
            "                                                                                                  \n",
            " price (Dense)                  (None, 3)            771         ['dense_1[0][0]']                \n",
            "                                                                                                  \n",
            " type (Dense)                   (None, 24)           6168        ['dense_1[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4,307,355\n",
            "Trainable params: 4,307,355\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training\n",
        "\n",
        "Based on the training/validation performance, you can adjust the epochs to be trained. Early stoping is watching the validation loss on genre prediction (assuming that it is the main task we would like to perform)"
      ],
      "metadata": {
        "id": "MxxPWXulbXr3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model5.fit(\n",
        "    x={\n",
        "        'summary': x_train_text_id\n",
        "           },\n",
        "    y={\n",
        "        'type': y_train_type,\n",
        "        'price': y_train_price,\n",
        "    },\n",
        "    epochs=20,\n",
        "    batch_size=16,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_price_loss', patience=5,)],\n",
        "    verbose=1\n",
        ")\n",
        "     "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBYx2OZYafeC",
        "outputId": "65c9dd60-a9a2-4a00-89c7-7ff84446e5a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "382/382 [==============================] - 72s 179ms/step - loss: 0.9581 - price_loss: 0.8468 - type_loss: 1.0695 - price_sparse_categorical_accuracy: 0.6184 - type_sparse_categorical_accuracy: 0.7507 - val_loss: 0.8726 - val_price_loss: 0.8133 - val_type_loss: 0.9318 - val_price_sparse_categorical_accuracy: 0.6252 - val_type_sparse_categorical_accuracy: 0.7647\n",
            "Epoch 2/20\n",
            "382/382 [==============================] - 63s 166ms/step - loss: 0.8812 - price_loss: 0.8061 - type_loss: 0.9563 - price_sparse_categorical_accuracy: 0.6232 - type_sparse_categorical_accuracy: 0.7525 - val_loss: 0.8601 - val_price_loss: 0.7944 - val_type_loss: 0.9258 - val_price_sparse_categorical_accuracy: 0.6298 - val_type_sparse_categorical_accuracy: 0.7647\n",
            "Epoch 3/20\n",
            "382/382 [==============================] - 70s 182ms/step - loss: 0.8152 - price_loss: 0.7694 - type_loss: 0.8609 - price_sparse_categorical_accuracy: 0.6310 - type_sparse_categorical_accuracy: 0.7550 - val_loss: 0.8359 - val_price_loss: 0.7743 - val_type_loss: 0.8974 - val_price_sparse_categorical_accuracy: 0.6435 - val_type_sparse_categorical_accuracy: 0.7621\n",
            "Epoch 4/20\n",
            "382/382 [==============================] - 64s 166ms/step - loss: 0.7488 - price_loss: 0.7198 - type_loss: 0.7777 - price_sparse_categorical_accuracy: 0.6602 - type_sparse_categorical_accuracy: 0.7725 - val_loss: 0.8014 - val_price_loss: 0.7426 - val_type_loss: 0.8602 - val_price_sparse_categorical_accuracy: 0.6632 - val_type_sparse_categorical_accuracy: 0.7798\n",
            "Epoch 5/20\n",
            "382/382 [==============================] - 62s 163ms/step - loss: 0.6667 - price_loss: 0.6489 - type_loss: 0.6845 - price_sparse_categorical_accuracy: 0.7202 - type_sparse_categorical_accuracy: 0.7963 - val_loss: 0.7989 - val_price_loss: 0.7520 - val_type_loss: 0.8459 - val_price_sparse_categorical_accuracy: 0.6442 - val_type_sparse_categorical_accuracy: 0.7785\n",
            "Epoch 6/20\n",
            "382/382 [==============================] - 64s 168ms/step - loss: 0.5992 - price_loss: 0.5956 - type_loss: 0.6029 - price_sparse_categorical_accuracy: 0.7509 - type_sparse_categorical_accuracy: 0.8218 - val_loss: 0.8508 - val_price_loss: 0.7761 - val_type_loss: 0.9256 - val_price_sparse_categorical_accuracy: 0.6409 - val_type_sparse_categorical_accuracy: 0.7713\n",
            "Epoch 7/20\n",
            "382/382 [==============================] - 64s 168ms/step - loss: 0.5397 - price_loss: 0.5461 - type_loss: 0.5333 - price_sparse_categorical_accuracy: 0.7691 - type_sparse_categorical_accuracy: 0.8407 - val_loss: 0.9307 - val_price_loss: 0.8755 - val_type_loss: 0.9859 - val_price_sparse_categorical_accuracy: 0.6638 - val_type_sparse_categorical_accuracy: 0.7661\n",
            "Epoch 8/20\n",
            "382/382 [==============================] - 66s 172ms/step - loss: 0.4845 - price_loss: 0.5006 - type_loss: 0.4683 - price_sparse_categorical_accuracy: 0.7905 - type_sparse_categorical_accuracy: 0.8584 - val_loss: 0.9464 - val_price_loss: 0.8812 - val_type_loss: 1.0117 - val_price_sparse_categorical_accuracy: 0.6520 - val_type_sparse_categorical_accuracy: 0.7608\n",
            "Epoch 9/20\n",
            "382/382 [==============================] - 63s 164ms/step - loss: 0.4410 - price_loss: 0.4598 - type_loss: 0.4222 - price_sparse_categorical_accuracy: 0.8064 - type_sparse_categorical_accuracy: 0.8700 - val_loss: 1.1197 - val_price_loss: 1.0403 - val_type_loss: 1.1992 - val_price_sparse_categorical_accuracy: 0.6547 - val_type_sparse_categorical_accuracy: 0.7674\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predition\n",
        "\n",
        "We can use the model to predict the testing samples."
      ],
      "metadata": {
        "id": "5pt2SLQzbdo3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "predicting both price and type"
      ],
      "metadata": {
        "id": "4J4LauQzbtgo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we can do prediction on training set\n",
        "y_predict5 = model5.predict(\n",
        "    {\n",
        "    'summary':x_test_text\n",
        "    }\n",
        "  )\n",
        "\n",
        "# probabilities\n",
        "type_predicted5 = y_predict5['type']\n",
        "print(type_predicted5)\n",
        "\n",
        "type_category_predicted5 = np.argmax(type_predicted5, axis=1)\n",
        "print(type_category_predicted5)\n",
        "\n",
        "price_predicted5 = y_predict5['price']\n",
        "print(price_predicted5)\n",
        "\n",
        "price_category_predicted5 = np.argmax(price_predicted5, axis=1)\n",
        "print(price_category_predicted5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "na9IUnz2argP",
        "outputId": "900c701c-4d5a-4cb0-ff80-bcf4eb47c69e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "230/230 [==============================] - 15s 65ms/step\n",
            "[[3.5080023e-04 8.8914065e-03 4.3359105e-03 ... 9.3631074e-04\n",
            "  3.8276961e-01 3.9619301e-03]\n",
            " [2.4996700e-08 9.7961354e-01 1.9022232e-03 ... 8.1066055e-06\n",
            "  2.5707381e-04 2.5596793e-05]\n",
            " [1.9275471e-04 3.0501368e-02 1.9963922e-02 ... 3.3664291e-03\n",
            "  1.8065031e-01 1.0018276e-02]\n",
            " ...\n",
            " [1.2748723e-05 8.5899621e-01 7.8031225e-03 ... 3.1685393e-04\n",
            "  4.3612630e-03 6.3309749e-04]\n",
            " [1.0225508e-16 9.9890971e-01 3.2342257e-05 ... 1.0499946e-10\n",
            "  7.4502367e-07 3.4765608e-09]\n",
            " [1.1360447e-06 8.3666897e-01 4.2780614e-04 ... 1.3652410e-05\n",
            "  2.7824740e-03 4.9622926e-05]]\n",
            "[17  1 17 ...  1  1  1]\n",
            "[[3.2998449e-01 5.0754577e-01 1.6246971e-01]\n",
            " [9.9166697e-01 8.1161801e-03 2.1677602e-04]\n",
            " [8.8235360e-01 1.0215426e-01 1.5492065e-02]\n",
            " ...\n",
            " [8.0689383e-01 1.7480882e-01 1.8297441e-02]\n",
            " [9.9999958e-01 3.8265790e-07 7.8640920e-11]\n",
            " [1.9826226e-01 6.9813508e-01 1.0360267e-01]]\n",
            "[1 0 0 ... 0 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  (if for kaggle competition and it is about genre prediction)\n",
        "pd.DataFrame(\n",
        "    {'id': test.id,\n",
        "     'price': price_category_predicted5}\n",
        ").to_csv('sample_submission5.csv', index=False)"
      ],
      "metadata": {
        "id": "rqmkwC_XbjZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This Trail Get  Score on Kaggle (0.64048)"
      ],
      "metadata": {
        "id": "2JWgoklHcRbm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* Multi-objective learning (multi-task) and single input can be more efficient and simpler, as it only requires one input data source and one model architecture. However, it may also face some challenges, such as balancing the trade-offs between different tasks, finding a common representation that suits all tasks, and avoiding negative transfer or interference between tasks. \n",
        "* Multi-modality learning and single task can be more effective and robust, as it can leverage the complementary and redundant information from different input data sources and improve the performance or generalization of the model. However, it may also face some challenges, such as aligning and fusing different modalities, dealing with missing or noisy data, and increasing the complexity and computation of the model.\n",
        "* I noticed that when using Multi-objective learning the model get accuracy better than when using Multi-modality learning with Multi-objective learning.\n",
        "i think that because the text data more clearly and better than image data in this model."
      ],
      "metadata": {
        "id": "a7Aa66SaYjOr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#The sixh Trail "
      ],
      "metadata": {
        "id": "1HaLpPe4gNhm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi-objective learning (multi-task)\n",
        "It's take image as an input and predict price and type as output."
      ],
      "metadata": {
        "id": "gkDDW7FGgcPl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Conv2d layer.\n"
      ],
      "metadata": {
        "id": "IZP_ZA2qhOu2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40479651-e0a2-4604-9a4e-a09f322fdeb6",
        "id": "0AVT8WkMhOu2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_6\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 64, 64, 2)]  0           []                               \n",
            "                                                                                                  \n",
            " conv2d_17 (Conv2D)             (None, 57, 57, 32)   4128        ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " max_pooling2d_12 (MaxPooling2D  (None, 7, 7, 32)    0           ['conv2d_17[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " dropout_12 (Dropout)           (None, 7, 7, 32)     0           ['max_pooling2d_12[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_18 (Conv2D)             (None, 4, 4, 64)     32832       ['dropout_12[0][0]']             \n",
            "                                                                                                  \n",
            " max_pooling2d_13 (MaxPooling2D  (None, 1, 1, 64)    0           ['conv2d_18[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " dropout_13 (Dropout)           (None, 1, 1, 64)     0           ['max_pooling2d_13[0][0]']       \n",
            "                                                                                                  \n",
            " flatten_4 (Flatten)            (None, 64)           0           ['dropout_13[0][0]']             \n",
            "                                                                                                  \n",
            " price (Dense)                  (None, 3)            195         ['flatten_4[0][0]']              \n",
            "                                                                                                  \n",
            " type (Dense)                   (None, 24)           1560        ['flatten_4[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 38,715\n",
            "Trainable params: 38,715\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# image part \n",
        "# simple conv2d. you can change it to anything else as needed\n",
        "cov1 = Conv2D(32, (8, 8),activation='relu',kernel_regularizer=regularizers.l2(0.01),kernel_initializer='he_uniform')(in_image)\n",
        "pl = MaxPool2D((8, 8))(cov1)\n",
        "d1=Dropout(0.25)(pl)\n",
        "cov2 = Conv2D(64, (4, 4),kernel_regularizer=regularizers.l2(0.01),activation='relu')(d1)\n",
        "p2 = MaxPool2D((4, 4))(cov2)\n",
        "d2=Dropout(0.25)(p2)\n",
        "flattened = Flatten()(d2)\n",
        "\n",
        "\n",
        "\n",
        "# multi-task learning (each is a multi-class classification)\n",
        "# one dense layer for each task\n",
        "p_type = Dense(len_type, activation='softmax', name='type')(flattened)\n",
        "p_price = Dense(len_price, activation='softmax', name='price')(flattened)\n",
        "\n",
        "\n",
        "# define model input/output using keys.\n",
        "model6 = keras.Model(\n",
        "    inputs={\n",
        "        'image': in_image\n",
        "    },\n",
        "    outputs={\n",
        "        'type': p_type,\n",
        "        'price': p_price,\n",
        "    },\n",
        ")\n",
        "\n",
        "\n",
        "# compile model with optimizer, loss values for each task, loss \n",
        "# weights for each task.\n",
        "model6.compile(\n",
        "    optimizer=Adam(lr=0.001),\n",
        "    loss={\n",
        "        'type': 'sparse_categorical_crossentropy',\n",
        "        'price': 'sparse_categorical_crossentropy',\n",
        "    },\n",
        "    loss_weights={\n",
        "        'type': 0.5,\n",
        "        'price': 0.5,       \n",
        "    },\n",
        "    metrics={\n",
        "        'type': ['SparseCategoricalAccuracy'],\n",
        "        'price': ['SparseCategoricalAccuracy'],\n",
        "    },\n",
        ")\n",
        "\n",
        "\n",
        "model6.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training\n",
        "\n",
        "Based on the training/validation performance, you can adjust the epochs to be trained. Early stoping is watching the validation loss on genre prediction (assuming that it is the main task we would like to perform)"
      ],
      "metadata": {
        "id": "XsMGah5-hOu3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a405f244-e35c-483f-cea4-04bb0e38609e",
        "id": "Je02ocWHhOu3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "381/382 [============================>.] - ETA: 0s - loss: 1.1394 - price_loss: 0.8439 - type_loss: 1.0269 - price_sparse_categorical_accuracy: 0.6199 - type_sparse_categorical_accuracy: 0.7536"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 22s 57ms/step - loss: 1.1394 - price_loss: 0.8437 - type_loss: 1.0272 - price_sparse_categorical_accuracy: 0.6201 - type_sparse_categorical_accuracy: 0.7536 - val_loss: 1.0869 - val_price_loss: 0.8317 - val_type_loss: 0.9722 - val_price_sparse_categorical_accuracy: 0.6252 - val_type_sparse_categorical_accuracy: 0.7647\n",
            "Epoch 2/20\n",
            "381/382 [============================>.] - ETA: 0s - loss: 1.1023 - price_loss: 0.8392 - type_loss: 1.0315 - price_sparse_categorical_accuracy: 0.6202 - type_sparse_categorical_accuracy: 0.7536"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 20s 54ms/step - loss: 1.1023 - price_loss: 0.8394 - type_loss: 1.0312 - price_sparse_categorical_accuracy: 0.6202 - type_sparse_categorical_accuracy: 0.7536 - val_loss: 1.0511 - val_price_loss: 0.8312 - val_type_loss: 0.9724 - val_price_sparse_categorical_accuracy: 0.6252 - val_type_sparse_categorical_accuracy: 0.7647\n",
            "Epoch 3/20\n",
            "381/382 [============================>.] - ETA: 0s - loss: 1.0670 - price_loss: 0.8402 - type_loss: 1.0277 - price_sparse_categorical_accuracy: 0.6202 - type_sparse_categorical_accuracy: 0.7534"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 23s 60ms/step - loss: 1.0669 - price_loss: 0.8405 - type_loss: 1.0272 - price_sparse_categorical_accuracy: 0.6201 - type_sparse_categorical_accuracy: 0.7536 - val_loss: 1.0192 - val_price_loss: 0.8317 - val_type_loss: 0.9723 - val_price_sparse_categorical_accuracy: 0.6252 - val_type_sparse_categorical_accuracy: 0.7647\n",
            "Epoch 4/20\n",
            "381/382 [============================>.] - ETA: 0s - loss: 1.0374 - price_loss: 0.8411 - type_loss: 1.0281 - price_sparse_categorical_accuracy: 0.6199 - type_sparse_categorical_accuracy: 0.7538"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 25s 67ms/step - loss: 1.0375 - price_loss: 0.8409 - type_loss: 1.0285 - price_sparse_categorical_accuracy: 0.6201 - type_sparse_categorical_accuracy: 0.7536 - val_loss: 0.9906 - val_price_loss: 0.8312 - val_type_loss: 0.9718 - val_price_sparse_categorical_accuracy: 0.6252 - val_type_sparse_categorical_accuracy: 0.7647\n",
            "Epoch 5/20\n",
            "381/382 [============================>.] - ETA: 0s - loss: 1.0107 - price_loss: 0.8418 - type_loss: 1.0260 - price_sparse_categorical_accuracy: 0.6199 - type_sparse_categorical_accuracy: 0.7536"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 20s 53ms/step - loss: 1.0104 - price_loss: 0.8416 - type_loss: 1.0257 - price_sparse_categorical_accuracy: 0.6201 - type_sparse_categorical_accuracy: 0.7536 - val_loss: 0.9669 - val_price_loss: 0.8311 - val_type_loss: 0.9724 - val_price_sparse_categorical_accuracy: 0.6252 - val_type_sparse_categorical_accuracy: 0.7647\n",
            "Epoch 6/20\n",
            "381/382 [============================>.] - ETA: 0s - loss: 0.9883 - price_loss: 0.8402 - type_loss: 1.0263 - price_sparse_categorical_accuracy: 0.6199 - type_sparse_categorical_accuracy: 0.7534"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 22s 58ms/step - loss: 0.9879 - price_loss: 0.8400 - type_loss: 1.0256 - price_sparse_categorical_accuracy: 0.6201 - type_sparse_categorical_accuracy: 0.7536 - val_loss: 0.9479 - val_price_loss: 0.8317 - val_type_loss: 0.9727 - val_price_sparse_categorical_accuracy: 0.6252 - val_type_sparse_categorical_accuracy: 0.7647\n",
            "Epoch 7/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 0.9689 - price_loss: 0.8403 - type_loss: 1.0217 - price_sparse_categorical_accuracy: 0.6201 - type_sparse_categorical_accuracy: 0.7536"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 23s 60ms/step - loss: 0.9689 - price_loss: 0.8403 - type_loss: 1.0217 - price_sparse_categorical_accuracy: 0.6201 - type_sparse_categorical_accuracy: 0.7536 - val_loss: 0.9327 - val_price_loss: 0.8316 - val_type_loss: 0.9726 - val_price_sparse_categorical_accuracy: 0.6252 - val_type_sparse_categorical_accuracy: 0.7647\n",
            "Epoch 8/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 0.9581 - price_loss: 0.8402 - type_loss: 1.0264 - price_sparse_categorical_accuracy: 0.6201 - type_sparse_categorical_accuracy: 0.7536"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 21s 54ms/step - loss: 0.9581 - price_loss: 0.8402 - type_loss: 1.0264 - price_sparse_categorical_accuracy: 0.6201 - type_sparse_categorical_accuracy: 0.7536 - val_loss: 0.9220 - val_price_loss: 0.8312 - val_type_loss: 0.9740 - val_price_sparse_categorical_accuracy: 0.6252 - val_type_sparse_categorical_accuracy: 0.7647\n",
            "Epoch 9/20\n",
            "381/382 [============================>.] - ETA: 0s - loss: 0.9477 - price_loss: 0.8389 - type_loss: 1.0259 - price_sparse_categorical_accuracy: 0.6202 - type_sparse_categorical_accuracy: 0.7538"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 24s 62ms/step - loss: 0.9480 - price_loss: 0.8392 - type_loss: 1.0260 - price_sparse_categorical_accuracy: 0.6201 - type_sparse_categorical_accuracy: 0.7536 - val_loss: 0.9147 - val_price_loss: 0.8312 - val_type_loss: 0.9749 - val_price_sparse_categorical_accuracy: 0.6252 - val_type_sparse_categorical_accuracy: 0.7647\n",
            "Epoch 10/20\n",
            "381/382 [============================>.] - ETA: 0s - loss: 0.9392 - price_loss: 0.8382 - type_loss: 1.0224 - price_sparse_categorical_accuracy: 0.6204 - type_sparse_categorical_accuracy: 0.7538"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 22s 59ms/step - loss: 0.9395 - price_loss: 0.8384 - type_loss: 1.0228 - price_sparse_categorical_accuracy: 0.6201 - type_sparse_categorical_accuracy: 0.7536 - val_loss: 0.9096 - val_price_loss: 0.8313 - val_type_loss: 0.9749 - val_price_sparse_categorical_accuracy: 0.6252 - val_type_sparse_categorical_accuracy: 0.7647\n",
            "Epoch 11/20\n",
            "381/382 [============================>.] - ETA: 0s - loss: 0.9365 - price_loss: 0.8393 - type_loss: 1.0240 - price_sparse_categorical_accuracy: 0.6199 - type_sparse_categorical_accuracy: 0.7534"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 20s 53ms/step - loss: 0.9361 - price_loss: 0.8391 - type_loss: 1.0234 - price_sparse_categorical_accuracy: 0.6201 - type_sparse_categorical_accuracy: 0.7536 - val_loss: 0.9049 - val_price_loss: 0.8311 - val_type_loss: 0.9719 - val_price_sparse_categorical_accuracy: 0.6252 - val_type_sparse_categorical_accuracy: 0.7647\n",
            "Epoch 12/20\n",
            "381/382 [============================>.] - ETA: 0s - loss: 0.9347 - price_loss: 0.8389 - type_loss: 1.0257 - price_sparse_categorical_accuracy: 0.6199 - type_sparse_categorical_accuracy: 0.7536"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 22s 57ms/step - loss: 0.9345 - price_loss: 0.8387 - type_loss: 1.0256 - price_sparse_categorical_accuracy: 0.6201 - type_sparse_categorical_accuracy: 0.7536 - val_loss: 0.9036 - val_price_loss: 0.8313 - val_type_loss: 0.9728 - val_price_sparse_categorical_accuracy: 0.6252 - val_type_sparse_categorical_accuracy: 0.7647\n",
            "Epoch 13/20\n",
            "381/382 [============================>.] - ETA: 0s - loss: 0.9311 - price_loss: 0.8377 - type_loss: 1.0225 - price_sparse_categorical_accuracy: 0.6199 - type_sparse_categorical_accuracy: 0.7534"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 21s 54ms/step - loss: 0.9307 - price_loss: 0.8375 - type_loss: 1.0219 - price_sparse_categorical_accuracy: 0.6201 - type_sparse_categorical_accuracy: 0.7536 - val_loss: 0.9036 - val_price_loss: 0.8312 - val_type_loss: 0.9746 - val_price_sparse_categorical_accuracy: 0.6252 - val_type_sparse_categorical_accuracy: 0.7647\n",
            "Epoch 14/20\n",
            "381/382 [============================>.] - ETA: 0s - loss: 0.9299 - price_loss: 0.8394 - type_loss: 1.0196 - price_sparse_categorical_accuracy: 0.6201 - type_sparse_categorical_accuracy: 0.7538"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 22s 58ms/step - loss: 0.9299 - price_loss: 0.8393 - type_loss: 1.0197 - price_sparse_categorical_accuracy: 0.6201 - type_sparse_categorical_accuracy: 0.7536 - val_loss: 0.9029 - val_price_loss: 0.8314 - val_type_loss: 0.9738 - val_price_sparse_categorical_accuracy: 0.6252 - val_type_sparse_categorical_accuracy: 0.7647\n",
            "Epoch 15/20\n",
            "381/382 [============================>.] - ETA: 0s - loss: 0.9318 - price_loss: 0.8390 - type_loss: 1.0243 - price_sparse_categorical_accuracy: 0.6201 - type_sparse_categorical_accuracy: 0.7536"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 22s 57ms/step - loss: 0.9316 - price_loss: 0.8390 - type_loss: 1.0240 - price_sparse_categorical_accuracy: 0.6201 - type_sparse_categorical_accuracy: 0.7536 - val_loss: 0.9038 - val_price_loss: 0.8311 - val_type_loss: 0.9763 - val_price_sparse_categorical_accuracy: 0.6252 - val_type_sparse_categorical_accuracy: 0.7647\n",
            "Epoch 16/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 0.9305 - price_loss: 0.8385 - type_loss: 1.0224 - price_sparse_categorical_accuracy: 0.6201 - type_sparse_categorical_accuracy: 0.7536"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 20s 53ms/step - loss: 0.9305 - price_loss: 0.8385 - type_loss: 1.0224 - price_sparse_categorical_accuracy: 0.6201 - type_sparse_categorical_accuracy: 0.7536 - val_loss: 0.9028 - val_price_loss: 0.8312 - val_type_loss: 0.9743 - val_price_sparse_categorical_accuracy: 0.6252 - val_type_sparse_categorical_accuracy: 0.7647\n",
            "Epoch 17/20\n",
            "381/382 [============================>.] - ETA: 0s - loss: 0.9302 - price_loss: 0.8387 - type_loss: 1.0217 - price_sparse_categorical_accuracy: 0.6198 - type_sparse_categorical_accuracy: 0.7538"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 22s 57ms/step - loss: 0.9303 - price_loss: 0.8384 - type_loss: 1.0222 - price_sparse_categorical_accuracy: 0.6201 - type_sparse_categorical_accuracy: 0.7536 - val_loss: 0.9023 - val_price_loss: 0.8313 - val_type_loss: 0.9734 - val_price_sparse_categorical_accuracy: 0.6252 - val_type_sparse_categorical_accuracy: 0.7647\n",
            "Epoch 18/20\n",
            "381/382 [============================>.] - ETA: 0s - loss: 0.9294 - price_loss: 0.8377 - type_loss: 1.0210 - price_sparse_categorical_accuracy: 0.6202 - type_sparse_categorical_accuracy: 0.7538"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 21s 54ms/step - loss: 0.9295 - price_loss: 0.8378 - type_loss: 1.0211 - price_sparse_categorical_accuracy: 0.6201 - type_sparse_categorical_accuracy: 0.7536 - val_loss: 0.9030 - val_price_loss: 0.8321 - val_type_loss: 0.9739 - val_price_sparse_categorical_accuracy: 0.6252 - val_type_sparse_categorical_accuracy: 0.7647\n",
            "Epoch 19/20\n",
            "382/382 [==============================] - ETA: 0s - loss: 0.9293 - price_loss: 0.8373 - type_loss: 1.0214 - price_sparse_categorical_accuracy: 0.6201 - type_sparse_categorical_accuracy: 0.7536"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 22s 57ms/step - loss: 0.9293 - price_loss: 0.8373 - type_loss: 1.0214 - price_sparse_categorical_accuracy: 0.6201 - type_sparse_categorical_accuracy: 0.7536 - val_loss: 0.9043 - val_price_loss: 0.8341 - val_type_loss: 0.9745 - val_price_sparse_categorical_accuracy: 0.6252 - val_type_sparse_categorical_accuracy: 0.7647\n",
            "Epoch 20/20\n",
            "381/382 [============================>.] - ETA: 0s - loss: 0.9297 - price_loss: 0.8392 - type_loss: 1.0203 - price_sparse_categorical_accuracy: 0.6201 - type_sparse_categorical_accuracy: 0.7534"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 23s 61ms/step - loss: 0.9294 - price_loss: 0.8391 - type_loss: 1.0197 - price_sparse_categorical_accuracy: 0.6201 - type_sparse_categorical_accuracy: 0.7536 - val_loss: 0.9038 - val_price_loss: 0.8312 - val_type_loss: 0.9764 - val_price_sparse_categorical_accuracy: 0.6252 - val_type_sparse_categorical_accuracy: 0.7647\n"
          ]
        }
      ],
      "source": [
        "history = model6.fit(\n",
        "    x={\n",
        "        'image': x_train_image\n",
        "    },\n",
        "    y={\n",
        "        'type': y_train_type,\n",
        "        'price': y_train_price,\n",
        "    },\n",
        "    epochs=20,\n",
        "    batch_size=16,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_genre_loss', patience=5, )\n",
        "    ],\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predition\n",
        "\n",
        "We can use the model to predict the testing samples."
      ],
      "metadata": {
        "id": "gCACar-hhOu3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "937727d6-4086-4b56-ddad-d7825183dc42",
        "id": "jDWhSRhvhOu3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "230/230 [==============================] - 5s 22ms/step\n",
            "[[0.61702    0.3165734  0.06640667]\n",
            " [0.61702    0.3165734  0.06640667]\n",
            " [0.61702    0.3165734  0.06640667]\n",
            " ...\n",
            " [0.61702    0.3165734  0.06640667]\n",
            " [0.61702    0.3165734  0.06640667]\n",
            " [0.61702    0.3165734  0.06640667]]\n",
            "[0 0 0 ... 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "# we can do prediction on training set\n",
        "y_predict6 = model6.predict(\n",
        "    {\n",
        "        'image': x_test_image\n",
        "    }\n",
        ")\n",
        "\n",
        "\n",
        "# probabilities\n",
        "price_predicted6 = y_predict6['price']\n",
        "print(price_predicted6)\n",
        "\n",
        "# categories\n",
        "price_category_predicted6 = np.argmax(price_predicted6, axis=1)\n",
        "print(price_category_predicted6)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRVBCE6FhOu3"
      },
      "outputs": [],
      "source": [
        "#  (if for kaggle competition and it is about genre prediction)\n",
        "pd.DataFrame(\n",
        "    {'id': test.id,\n",
        "     'price': price_category_predicted6}\n",
        ").to_csv('sample_submission6.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This Trail Get  Score on Kaggle (0.62038)\n"
      ],
      "metadata": {
        "id": "3PRuQaMghOu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I don't notice much change in the account compared to the previous model which I used a conv2d layer in it."
      ],
      "metadata": {
        "id": "rTyiQlJvZaYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Seventh trail\n"
      ],
      "metadata": {
        "id": "V-kvpccui0DC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `Transfer learning`\n",
        "Using VGG model: \n",
        "\n",
        "*   this method has faster training speed, fewer training samples per time, and higher accuracy.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KbPuoiMuoi8c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi-modality learning with Multi-objective learning "
      ],
      "metadata": {
        "id": "AxlZ2-u3kULk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import VGG19\n",
        "\n",
        "# text part\n",
        "# simple average of embedding.\n",
        "embedded = keras.layers.Embedding(tokenizer.num_words, 200)(in_text)\n",
        "averaged = tf.reduce_mean(embedded, axis=1)\n",
        "\n",
        "\n",
        "# image part \n",
        "# using conv2d\n",
        "cov = Conv2D(32,(8,8), activation='tanh')(in_image) # 32 number of filters  and  (8, 8) size of filter\n",
        "vgg=VGG19(weights=None, input_shape=(57, 57, 32), include_top=False)(cov)\n",
        "flattened = Flatten()(vgg)\n",
        "\n",
        "# fusion - combinig both\n",
        "fused = tf.concat([averaged, flattened], axis=-1)\n",
        "\n",
        "# multi-task learning (each is a multi-class classification)\n",
        "# one dense layer for each task\n",
        "p_type = Dense(len_type, activation='softmax', name='type')(fused)\n",
        "p_price = Dense(len_price, activation='softmax', name='price')(fused)\n",
        "\n",
        "\n",
        "# define model input/output using keys.\n",
        "model7 = keras.Model(\n",
        "    inputs={\n",
        "        'summary': in_text,\n",
        "        'image': in_image\n",
        "    },\n",
        "    outputs={\n",
        "        'type': p_type,\n",
        "        'price': p_price,\n",
        "    },\n",
        ")\n",
        "\n",
        "\n",
        "# compile model with optimizer, loss values for each task, loss \n",
        "# weights for each task.\n",
        "model7.compile(\n",
        "    optimizer=Adam(),\n",
        "    loss={\n",
        "        'type': 'sparse_categorical_crossentropy',\n",
        "        'price': 'sparse_categorical_crossentropy',\n",
        "    },\n",
        "    loss_weights={\n",
        "        'type': 0.7,\n",
        "        'price': 0.3,       \n",
        "    },\n",
        "    metrics={\n",
        "        'type': ['SparseCategoricalAccuracy'],\n",
        "        'price': ['SparseCategoricalAccuracy'],\n",
        "    },\n",
        ")\n",
        "\n",
        "\n",
        "model7.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVA1-j7AjYyn",
        "outputId": "65173c62-4023-45b7-ac63-27e144b48c37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/applications/vgg19.py:137: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed an input_shape with 32 input channels.\n",
            "  input_shape = imagenet_utils.obtain_input_shape(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 64, 64, 2)]  0           []                               \n",
            "                                                                                                  \n",
            " input_1 (InputLayer)           [(None, 100)]        0           []                               \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)              (None, 57, 57, 32)   4128        ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_5 (Embedding)        (None, 100, 200)     8000000     ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " vgg19 (Functional)             (None, 1, 1, 512)    20041088    ['conv2d_4[0][0]']               \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_3 (TFOpLam  (None, 200)         0           ['embedding_5[0][0]']            \n",
            " bda)                                                                                             \n",
            "                                                                                                  \n",
            " flatten_3 (Flatten)            (None, 512)          0           ['vgg19[0][0]']                  \n",
            "                                                                                                  \n",
            " tf.concat_3 (TFOpLambda)       (None, 712)          0           ['tf.math.reduce_mean_3[0][0]',  \n",
            "                                                                  'flatten_3[0][0]']              \n",
            "                                                                                                  \n",
            " price (Dense)                  (None, 3)            2139        ['tf.concat_3[0][0]']            \n",
            "                                                                                                  \n",
            " type (Dense)                   (None, 24)           17112       ['tf.concat_3[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 28,064,467\n",
            "Trainable params: 28,064,467\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training\n",
        "\n",
        "Based on the training/validation performance, you can adjust the epochs to be trained. Early stoping is watching the validation loss on genre prediction (assuming that it is the main task we would like to perform)"
      ],
      "metadata": {
        "id": "GTAVCJi0rKkk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70fdbcf2-438f-48ee-f067-f9f6f487992d",
        "id": "nC3k3kTVrKkl"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/12\n",
            "382/382 [==============================] - ETA: 0s - loss: 0.0863 - price_loss: 0.1126 - type_loss: 0.0750 - price_sparse_categorical_accuracy: 0.9567 - type_sparse_categorical_accuracy: 0.9785"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 19s 49ms/step - loss: 0.0863 - price_loss: 0.1126 - type_loss: 0.0750 - price_sparse_categorical_accuracy: 0.9567 - type_sparse_categorical_accuracy: 0.9785 - val_loss: 1.7140 - val_price_loss: 1.8226 - val_type_loss: 1.6675 - val_price_sparse_categorical_accuracy: 0.6186 - val_type_sparse_categorical_accuracy: 0.7726\n",
            "Epoch 2/12\n",
            "382/382 [==============================] - ETA: 0s - loss: 0.0877 - price_loss: 0.1115 - type_loss: 0.0774 - price_sparse_categorical_accuracy: 0.9571 - type_sparse_categorical_accuracy: 0.9797"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 18s 48ms/step - loss: 0.0877 - price_loss: 0.1115 - type_loss: 0.0774 - price_sparse_categorical_accuracy: 0.9571 - type_sparse_categorical_accuracy: 0.9797 - val_loss: 1.7370 - val_price_loss: 1.8625 - val_type_loss: 1.6832 - val_price_sparse_categorical_accuracy: 0.5885 - val_type_sparse_categorical_accuracy: 0.7706\n",
            "Epoch 3/12\n",
            "382/382 [==============================] - ETA: 0s - loss: 0.0840 - price_loss: 0.1076 - type_loss: 0.0738 - price_sparse_categorical_accuracy: 0.9589 - type_sparse_categorical_accuracy: 0.9792"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 18s 47ms/step - loss: 0.0840 - price_loss: 0.1076 - type_loss: 0.0738 - price_sparse_categorical_accuracy: 0.9589 - type_sparse_categorical_accuracy: 0.9792 - val_loss: 1.7633 - val_price_loss: 1.8717 - val_type_loss: 1.7168 - val_price_sparse_categorical_accuracy: 0.6186 - val_type_sparse_categorical_accuracy: 0.7726\n",
            "Epoch 4/12\n",
            "381/382 [============================>.] - ETA: 0s - loss: 0.0828 - price_loss: 0.1062 - type_loss: 0.0728 - price_sparse_categorical_accuracy: 0.9590 - type_sparse_categorical_accuracy: 0.9792"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 18s 48ms/step - loss: 0.0829 - price_loss: 0.1064 - type_loss: 0.0728 - price_sparse_categorical_accuracy: 0.9589 - type_sparse_categorical_accuracy: 0.9792 - val_loss: 1.7594 - val_price_loss: 1.8928 - val_type_loss: 1.7022 - val_price_sparse_categorical_accuracy: 0.6199 - val_type_sparse_categorical_accuracy: 0.7634\n",
            "Epoch 5/12\n",
            "382/382 [==============================] - ETA: 0s - loss: 0.0824 - price_loss: 0.1074 - type_loss: 0.0717 - price_sparse_categorical_accuracy: 0.9582 - type_sparse_categorical_accuracy: 0.9800"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 18s 48ms/step - loss: 0.0824 - price_loss: 0.1074 - type_loss: 0.0717 - price_sparse_categorical_accuracy: 0.9582 - type_sparse_categorical_accuracy: 0.9800 - val_loss: 1.8077 - val_price_loss: 1.9299 - val_type_loss: 1.7552 - val_price_sparse_categorical_accuracy: 0.6081 - val_type_sparse_categorical_accuracy: 0.7071\n",
            "Epoch 6/12\n",
            "381/382 [============================>.] - ETA: 0s - loss: 0.0840 - price_loss: 0.1059 - type_loss: 0.0745 - price_sparse_categorical_accuracy: 0.9592 - type_sparse_categorical_accuracy: 0.9782"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 18s 47ms/step - loss: 0.0839 - price_loss: 0.1059 - type_loss: 0.0745 - price_sparse_categorical_accuracy: 0.9592 - type_sparse_categorical_accuracy: 0.9782 - val_loss: 1.8138 - val_price_loss: 1.9532 - val_type_loss: 1.7541 - val_price_sparse_categorical_accuracy: 0.6212 - val_type_sparse_categorical_accuracy: 0.7700\n",
            "Epoch 7/12\n",
            "381/382 [============================>.] - ETA: 0s - loss: 0.0818 - price_loss: 0.1016 - type_loss: 0.0733 - price_sparse_categorical_accuracy: 0.9610 - type_sparse_categorical_accuracy: 0.9795"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 18s 48ms/step - loss: 0.0818 - price_loss: 0.1016 - type_loss: 0.0733 - price_sparse_categorical_accuracy: 0.9610 - type_sparse_categorical_accuracy: 0.9795 - val_loss: 1.8239 - val_price_loss: 1.9972 - val_type_loss: 1.7497 - val_price_sparse_categorical_accuracy: 0.6160 - val_type_sparse_categorical_accuracy: 0.7661\n",
            "Epoch 8/12\n",
            "382/382 [==============================] - ETA: 0s - loss: 0.0802 - price_loss: 0.1004 - type_loss: 0.0715 - price_sparse_categorical_accuracy: 0.9610 - type_sparse_categorical_accuracy: 0.9800"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 18s 47ms/step - loss: 0.0802 - price_loss: 0.1004 - type_loss: 0.0715 - price_sparse_categorical_accuracy: 0.9610 - type_sparse_categorical_accuracy: 0.9800 - val_loss: 1.8553 - val_price_loss: 2.0152 - val_type_loss: 1.7868 - val_price_sparse_categorical_accuracy: 0.6173 - val_type_sparse_categorical_accuracy: 0.7700\n",
            "Epoch 9/12\n",
            "382/382 [==============================] - ETA: 0s - loss: 0.0793 - price_loss: 0.1002 - type_loss: 0.0704 - price_sparse_categorical_accuracy: 0.9615 - type_sparse_categorical_accuracy: 0.9797"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 18s 47ms/step - loss: 0.0793 - price_loss: 0.1002 - type_loss: 0.0704 - price_sparse_categorical_accuracy: 0.9615 - type_sparse_categorical_accuracy: 0.9797 - val_loss: 1.8951 - val_price_loss: 2.0410 - val_type_loss: 1.8326 - val_price_sparse_categorical_accuracy: 0.6258 - val_type_sparse_categorical_accuracy: 0.7765\n",
            "Epoch 10/12\n",
            "382/382 [==============================] - ETA: 0s - loss: 0.0795 - price_loss: 0.0997 - type_loss: 0.0709 - price_sparse_categorical_accuracy: 0.9608 - type_sparse_categorical_accuracy: 0.9798"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 18s 48ms/step - loss: 0.0795 - price_loss: 0.0997 - type_loss: 0.0709 - price_sparse_categorical_accuracy: 0.9608 - type_sparse_categorical_accuracy: 0.9798 - val_loss: 1.9061 - val_price_loss: 2.1212 - val_type_loss: 1.8139 - val_price_sparse_categorical_accuracy: 0.5852 - val_type_sparse_categorical_accuracy: 0.7615\n",
            "Epoch 11/12\n",
            "382/382 [==============================] - ETA: 0s - loss: 0.0787 - price_loss: 0.0978 - type_loss: 0.0705 - price_sparse_categorical_accuracy: 0.9623 - type_sparse_categorical_accuracy: 0.9800"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 18s 47ms/step - loss: 0.0787 - price_loss: 0.0978 - type_loss: 0.0705 - price_sparse_categorical_accuracy: 0.9623 - type_sparse_categorical_accuracy: 0.9800 - val_loss: 1.8973 - val_price_loss: 2.1009 - val_type_loss: 1.8101 - val_price_sparse_categorical_accuracy: 0.6153 - val_type_sparse_categorical_accuracy: 0.7595\n",
            "Epoch 12/12\n",
            "382/382 [==============================] - ETA: 0s - loss: 0.0778 - price_loss: 0.0975 - type_loss: 0.0693 - price_sparse_categorical_accuracy: 0.9610 - type_sparse_categorical_accuracy: 0.9807"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_genre_loss` which is not available. Available metrics are: loss,price_loss,type_loss,price_sparse_categorical_accuracy,type_sparse_categorical_accuracy,val_loss,val_price_loss,val_type_loss,val_price_sparse_categorical_accuracy,val_type_sparse_categorical_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r382/382 [==============================] - 18s 47ms/step - loss: 0.0778 - price_loss: 0.0975 - type_loss: 0.0693 - price_sparse_categorical_accuracy: 0.9610 - type_sparse_categorical_accuracy: 0.9807 - val_loss: 1.9251 - val_price_loss: 2.1324 - val_type_loss: 1.8363 - val_price_sparse_categorical_accuracy: 0.6271 - val_type_sparse_categorical_accuracy: 0.7647\n"
          ]
        }
      ],
      "source": [
        "history = model7.fit(\n",
        "    x={\n",
        "        'summary': x_train_text_id,\n",
        "        'image': x_train_image\n",
        "    },\n",
        "    y={\n",
        "        'type': y_train_type,\n",
        "        'price': y_train_price,\n",
        "    },\n",
        "    epochs=10,\n",
        "    batch_size=16,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_genre_loss', patience=5, )\n",
        "    ],\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predition\n",
        "\n",
        "We can use the model to predict the testing samples."
      ],
      "metadata": {
        "id": "ZtKNrQ9BrKkl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mB1QJ_0ArKkl",
        "outputId": "72169736-3280-439d-c34f-f6dde4e3b893"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "230/230 [==============================] - 4s 17ms/step\n",
            "[[9.6809143e-01 3.1208266e-02 7.0031994e-04]\n",
            " [9.9816269e-01 8.2828001e-06 1.8290296e-03]\n",
            " [9.6217573e-01 1.2988654e-02 2.4835588e-02]\n",
            " ...\n",
            " [9.7351485e-01 2.3745116e-02 2.7400283e-03]\n",
            " [1.0000000e+00 3.9479387e-19 5.2057337e-16]\n",
            " [5.9652317e-01 3.9444959e-01 9.0273023e-03]]\n",
            "[0 0 0 ... 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "# we can do prediction on training set\n",
        "y_predict7 = model7.predict(\n",
        "    {\n",
        "        'summary': x_test_text,\n",
        "        'image': x_test_image\n",
        "    }\n",
        ")\n",
        "\n",
        "\n",
        "# probabilities\n",
        "price_predicted7 = y_predict7['price']\n",
        "print(price_predicted7)\n",
        "\n",
        "# categories\n",
        "price_category_predicted7 = np.argmax(price_predicted7, axis=1)\n",
        "print(price_category_predicted7)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-K8kg8KrKkl"
      },
      "outputs": [],
      "source": [
        "#  (if for kaggle competition and it is about genre prediction)\n",
        "pd.DataFrame(\n",
        "    {'id': test.id,\n",
        "     'price': price_category_predicted7}\n",
        ").to_csv('sample_submission7.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This Trail Get  Score on Kaggle (0.67527) (no. epoch =5)\n",
        "\n",
        "The score without Lammatization (0.67608) (no. epoch =5)\n",
        "\n",
        "The score without Lammatization (0.67934) (no. epoch =10)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "doAiPu2krKkl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "this trail get the best score in kaggle.\n"
      ],
      "metadata": {
        "id": "MCz_Idb1b1PI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "\n",
        "*   The use of multi modality allows the model to learn widely and thus predict more accurately.\n",
        "*   VGG get the best accuracy and this is the best model for dealing with image column in dataset.\n",
        "*   A better choice for text data would be an embedding layer, which can map the tokens or characters to a high-dimensional vector space, followed by a recurrent or attention layer, which can capture the sequential dependencies and semantics of the text.\n",
        "*   When I increase the number of epochs, the accuracy improves but if I increase it continually, the accuracy will decrease and it will be overfitting.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UlnyEHppcQb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úîÔ∏è Answer the questions"
      ],
      "metadata": {
        "id": "WiOA5ST3Ozd5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üåàIs fully-connected model a good one for sequential data? Why? How about for image data? Is it good? Why?**\n",
        "\n",
        "*  A fully-connected model is not a good choice for sequential data, such as text or speech, because it does not capture the temporal dependencies and order of the input. A better choice for sequential data would be a recurrent or convolutional model, which can learn from the previous states or local features of the input\n",
        "*  A fully-connected model can be used for image data, but it is not very efficient or effective. It requires a lot of parameters and computation, and it does not exploit the spatial structure and locality of the image pixels. A better choice for image data would be a convolutional model, which can learn from the filters and feature maps of the image.\n",
        "\n",
        "**üåàWhat is gradient vanishing and gradient explosion, and how GRU/LSTM tries to mitigate this problem?**\n",
        "\n",
        "*  Gradient vanishing and gradient explosion are two problems that occur when training recurrent neural networks (RNNs) with backpropagation through time (BPTT). Gradient vanishing means that the gradients become very small and close to zero as they propagate back through the network, making it hard to update the weights. Gradient explosion means that the gradients become very large and unstable as they propagate back through the network, causing numerical overflow and divergence.\n",
        "\n",
        "*  GRU (Gated Recurrent Unit) and LSTM (Long Short-Term Memory) are two types of RNNs that try to mitigate these problems by introducing gates. Gates are learnable units that control the flow of information inside the network. They can decide what to keep, what to forget, and what to update in the hidden state. By doing so, they can preserve the long-term dependencies and avoid the vanishing or exploding gradients.\n",
        "\n",
        "**üåàWhat is multi-objective/multi-task learning? What is multi-modality learning? How do you use them in this assignment?**\n",
        "\n",
        "\n",
        "*   Multi-objective learning is a type of learning that aims to optimize multiple objectives simultaneously, such as accuracy, fairness, robustness, etc. Multi-task learning is a special case of multi-objective learning, where the objectives are different tasks that share some common features or parameters.\n",
        "\n",
        "*   Multi-modality learning is a type of learning that deals with data that has multiple modalities, such as text, image, audio, video, etc. It tries to leverage the complementary and redundant information from different modalities to improve the performance or generalization of the model.\n",
        "\n",
        "*   In this assignment, when we use Multi-object learning, we have output as  type and price of real estates, and when we use Multi-Modality we have input as ummary and image of real estates. I combined these different learning methods, built a model, and simultaneously trained it on these inputs to be able to predict the values of type and price.\n",
        "\n",
        "\n",
        "**üåà What is the difference among xgboost, lightgbm and catboost?**\n",
        "\n",
        "\n",
        "*   XGBoost, LightGBM and CatBoost are three popular gradient boosting frameworks that can be used for classification and regression problems. They are based on the idea of building an ensemble of weak learners (decision trees) that are trained sequentially to minimize a loss function.\n",
        "\n",
        "    The main differences among them are:\n",
        "\n",
        "    *   XGBoost uses a level-wise tree growth strategy, which means it splits the tree level by level. This can result in a balanced and deep tree, but also more computation and memory usage.\n",
        "    *   LightGBM uses a leaf-wise tree growth strategy, which means it splits the tree by the leaf with the highest loss change. This can result in an unbalanced and shallow tree, but also faster training and lower memory usage.\n",
        "    *  CatBoost uses a symmetric tree growth strategy, which means it splits the tree by a feature across all levels. This can result in a balanced and shallow tree, but also less overfitting and better handling of categorical features.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pFcgWagyPAXX"
      }
    }
  ]
}